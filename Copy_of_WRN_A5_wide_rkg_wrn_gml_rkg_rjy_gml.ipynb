{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of WRN_A5_wide_rkg_wrn_gml_rkg_rjy_gml.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saicharan-r/EIP4/blob/master/Copy_of_WRN_A5_wide_rkg_wrn_gml_rkg_rjy_gml.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gyq8CE4ug5BK",
        "colab_type": "code",
        "outputId": "1347d058-7251-4993-9085-94d7852abb41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "# mount gdrive and unzip data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "!unzip -q \"/content/gdrive/My Drive/hvc_data.zip\"\n",
        "# look for `hvc_annotations.csv` file and `resized` dir\n",
        "%ls \n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "replace resized/9733.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "\u001b[0m\u001b[01;34mgdrive\u001b[0m/  hvc_annotations.csv  \u001b[01;34mresized\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYbNQzK6kj94",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "\n",
        "import cv2\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from functools import partial\n",
        "from pathlib import Path \n",
        "from tqdm import tqdm\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "\n",
        "from keras.applications import VGG16\n",
        "from keras.layers.core import Dropout\n",
        "from keras.layers.core import Flatten\n",
        "from keras.layers.core import Dense\n",
        "from keras.layers import Input\n",
        "from keras.models import Model\n",
        "from keras.optimizers import SGD\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mr6OZ3Li_RAJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "\n",
        "import tensorflow.contrib.eager as tfe\n",
        "#tf.enable_eager_execution()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQkbSpLK4sTP",
        "colab_type": "code",
        "outputId": "a3d387b9-3a0c-4370-9765-e18f84a69114",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "# load annotations\n",
        "df = pd.read_csv(\"hvc_annotations.csv\")\n",
        "del df[\"filename\"] # remove unwanted column\n",
        "df.head()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>gender</th>\n",
              "      <th>imagequality</th>\n",
              "      <th>age</th>\n",
              "      <th>weight</th>\n",
              "      <th>carryingbag</th>\n",
              "      <th>footwear</th>\n",
              "      <th>emotion</th>\n",
              "      <th>bodypose</th>\n",
              "      <th>image_path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>male</td>\n",
              "      <td>Average</td>\n",
              "      <td>35-45</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Grocery/Home/Plastic Bag</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/1.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>female</td>\n",
              "      <td>Average</td>\n",
              "      <td>35-45</td>\n",
              "      <td>over-weight</td>\n",
              "      <td>None</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Angry/Serious</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/2.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>male</td>\n",
              "      <td>Good</td>\n",
              "      <td>45-55</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Grocery/Home/Plastic Bag</td>\n",
              "      <td>CantSee</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/3.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>male</td>\n",
              "      <td>Good</td>\n",
              "      <td>45-55</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Daily/Office/Work Bag</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/4.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>female</td>\n",
              "      <td>Good</td>\n",
              "      <td>35-45</td>\n",
              "      <td>slightly-overweight</td>\n",
              "      <td>None</td>\n",
              "      <td>CantSee</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/5.jpg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   gender imagequality    age  ...        emotion        bodypose     image_path\n",
              "0    male      Average  35-45  ...        Neutral  Front-Frontish  resized/1.jpg\n",
              "1  female      Average  35-45  ...  Angry/Serious  Front-Frontish  resized/2.jpg\n",
              "2    male         Good  45-55  ...        Neutral  Front-Frontish  resized/3.jpg\n",
              "3    male         Good  45-55  ...        Neutral  Front-Frontish  resized/4.jpg\n",
              "4  female         Good  35-45  ...        Neutral  Front-Frontish  resized/5.jpg\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "202OJva345WA",
        "colab_type": "code",
        "outputId": "3261dd80-f4c0-449c-96b6-acd658f9b96b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 876
        }
      },
      "source": [
        "# one hot encoding of labels\n",
        "\n",
        "one_hot_df = pd.concat([\n",
        "    df[[\"image_path\"]],\n",
        "    pd.get_dummies(df.gender, prefix=\"gender\"),\n",
        "    pd.get_dummies(df.imagequality, prefix=\"imagequality\"),\n",
        "    pd.get_dummies(df.age, prefix=\"age\"),\n",
        "    pd.get_dummies(df.weight, prefix=\"weight\"),\n",
        "    pd.get_dummies(df.carryingbag, prefix=\"carryingbag\"),\n",
        "    pd.get_dummies(df.footwear, prefix=\"footwear\"),\n",
        "    pd.get_dummies(df.emotion, prefix=\"emotion\"),\n",
        "    pd.get_dummies(df.bodypose, prefix=\"bodypose\"),\n",
        "], axis = 1)\n",
        "\n",
        "one_hot_df.head().T"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>image_path</th>\n",
              "      <td>resized/1.jpg</td>\n",
              "      <td>resized/2.jpg</td>\n",
              "      <td>resized/3.jpg</td>\n",
              "      <td>resized/4.jpg</td>\n",
              "      <td>resized/5.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gender_female</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gender_male</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>imagequality_Average</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>imagequality_Bad</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>imagequality_Good</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_15-25</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_25-35</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_35-45</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_45-55</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_55+</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_normal-healthy</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_over-weight</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_slightly-overweight</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_underweight</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carryingbag_Daily/Office/Work Bag</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carryingbag_Grocery/Home/Plastic Bag</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carryingbag_None</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>footwear_CantSee</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>footwear_Fancy</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>footwear_Normal</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Angry/Serious</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Happy</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Neutral</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Sad</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bodypose_Back</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bodypose_Front-Frontish</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bodypose_Side</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  0  ...              4\n",
              "image_path                            resized/1.jpg  ...  resized/5.jpg\n",
              "gender_female                                     0  ...              1\n",
              "gender_male                                       1  ...              0\n",
              "imagequality_Average                              1  ...              0\n",
              "imagequality_Bad                                  0  ...              0\n",
              "imagequality_Good                                 0  ...              1\n",
              "age_15-25                                         0  ...              0\n",
              "age_25-35                                         0  ...              0\n",
              "age_35-45                                         1  ...              1\n",
              "age_45-55                                         0  ...              0\n",
              "age_55+                                           0  ...              0\n",
              "weight_normal-healthy                             1  ...              0\n",
              "weight_over-weight                                0  ...              0\n",
              "weight_slightly-overweight                        0  ...              1\n",
              "weight_underweight                                0  ...              0\n",
              "carryingbag_Daily/Office/Work Bag                 0  ...              0\n",
              "carryingbag_Grocery/Home/Plastic Bag              1  ...              0\n",
              "carryingbag_None                                  0  ...              1\n",
              "footwear_CantSee                                  0  ...              1\n",
              "footwear_Fancy                                    0  ...              0\n",
              "footwear_Normal                                   1  ...              0\n",
              "emotion_Angry/Serious                             0  ...              0\n",
              "emotion_Happy                                     0  ...              0\n",
              "emotion_Neutral                                   1  ...              1\n",
              "emotion_Sad                                       0  ...              0\n",
              "bodypose_Back                                     0  ...              0\n",
              "bodypose_Front-Frontish                           1  ...              1\n",
              "bodypose_Side                                     0  ...              0\n",
              "\n",
              "[28 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0PZZE92dNYM",
        "colab_type": "code",
        "outputId": "91a64ffe-1d16-4927-d1b4-c43ab0037d72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "source": [
        "one_hot_df.head()\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_path</th>\n",
              "      <th>gender_female</th>\n",
              "      <th>gender_male</th>\n",
              "      <th>imagequality_Average</th>\n",
              "      <th>imagequality_Bad</th>\n",
              "      <th>imagequality_Good</th>\n",
              "      <th>age_15-25</th>\n",
              "      <th>age_25-35</th>\n",
              "      <th>age_35-45</th>\n",
              "      <th>age_45-55</th>\n",
              "      <th>age_55+</th>\n",
              "      <th>weight_normal-healthy</th>\n",
              "      <th>weight_over-weight</th>\n",
              "      <th>weight_slightly-overweight</th>\n",
              "      <th>weight_underweight</th>\n",
              "      <th>carryingbag_Daily/Office/Work Bag</th>\n",
              "      <th>carryingbag_Grocery/Home/Plastic Bag</th>\n",
              "      <th>carryingbag_None</th>\n",
              "      <th>footwear_CantSee</th>\n",
              "      <th>footwear_Fancy</th>\n",
              "      <th>footwear_Normal</th>\n",
              "      <th>emotion_Angry/Serious</th>\n",
              "      <th>emotion_Happy</th>\n",
              "      <th>emotion_Neutral</th>\n",
              "      <th>emotion_Sad</th>\n",
              "      <th>bodypose_Back</th>\n",
              "      <th>bodypose_Front-Frontish</th>\n",
              "      <th>bodypose_Side</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>resized/1.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>resized/2.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>resized/3.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>resized/4.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>resized/5.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      image_path  gender_female  ...  bodypose_Front-Frontish  bodypose_Side\n",
              "0  resized/1.jpg              0  ...                        1              0\n",
              "1  resized/2.jpg              1  ...                        1              0\n",
              "2  resized/3.jpg              0  ...                        1              0\n",
              "3  resized/4.jpg              0  ...                        1              0\n",
              "4  resized/5.jpg              1  ...                        1              0\n",
              "\n",
              "[5 rows x 28 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ll94zTv6w5i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow\n",
        "from tensorflow.python.keras.utils.data_utils import Sequence\n",
        "\n",
        "# Label columns per attribute\n",
        "_gender_cols_ = [col for col in one_hot_df.columns if col.startswith(\"gender\")]\n",
        "_imagequality_cols_ = [col for col in one_hot_df.columns if col.startswith(\"imagequality\")]\n",
        "_age_cols_ = [col for col in one_hot_df.columns if col.startswith(\"age\")]\n",
        "_weight_cols_ = [col for col in one_hot_df.columns if col.startswith(\"weight\")]\n",
        "_carryingbag_cols_ = [col for col in one_hot_df.columns if col.startswith(\"carryingbag\")]\n",
        "_footwear_cols_ = [col for col in one_hot_df.columns if col.startswith(\"footwear\")]\n",
        "_emotion_cols_ = [col for col in one_hot_df.columns if col.startswith(\"emotion\")]\n",
        "_bodypose_cols_ = [col for col in one_hot_df.columns if col.startswith(\"bodypose\")]\n",
        "\n",
        "#class PersonDataGenerator(keras.utils.Sequence):\n",
        "class PersonDataGenerator(Sequence):\n",
        "    \"\"\"Ground truth data generator\"\"\"\n",
        "\n",
        "    \n",
        "    def __init__(self, df, batch_size=32, shuffle=True,normalize=False,aug_flow=None):\n",
        "        self.df = df\n",
        "        self.batch_size=batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.normalize = normalize\n",
        "        self.on_epoch_end()\n",
        "        self.aug_flow=aug_flow\n",
        "        #print(\"Shuffle = \",self.shuffle)\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.floor(self.df.shape[0] / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"fetch batched images and targets\"\"\"\n",
        "        batch_slice = slice(index * self.batch_size, (index + 1) * self.batch_size)\n",
        "        #print(batch_slice)\n",
        "        items = self.df.iloc[batch_slice]\n",
        "        image = np.stack([cv2.imread(item[\"image_path\"]) for _, item in items.iterrows()])\n",
        "        #print(items[\"image_path\"])\n",
        "        \n",
        "        target = {\n",
        "            \"gender_output\": items[_gender_cols_].values,\n",
        "            \"image_quality_output\": items[_imagequality_cols_].values,\n",
        "            \"age_output\": items[_age_cols_].values,\n",
        "            \"weight_output\": items[_weight_cols_].values,\n",
        "            \"bag_output\": items[_carryingbag_cols_].values,\n",
        "            \"pose_output\": items[_bodypose_cols_].values,\n",
        "            \"footwear_output\": items[_footwear_cols_].values,\n",
        "            \"emotion_output\": items[_emotion_cols_].values,\n",
        "        }\n",
        "        if(self.aug_flow is not None):\n",
        "            image = self.aug_flow.flow(image,shuffle=False,batch_size=self.batch_size).next()\n",
        "        if(self.normalize == True):\n",
        "            train_mean = np.mean(image, axis=(0,1,2))\n",
        "            train_std = np.std(image, axis=(0,1,2))\n",
        "            #print(train_mean, train_std)\n",
        "            normalize = lambda x: ((x - train_mean) / train_std).astype('float32')\n",
        "            image = normalize(image)\n",
        "\n",
        "\n",
        "        return image, target\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        \"\"\"Updates indexes after each epoch\"\"\"\n",
        "        if self.shuffle == True:\n",
        "            self.df = self.df.sample(frac=1).reset_index(drop=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVE8-OaZ8J5q",
        "colab_type": "code",
        "outputId": "32b9a42d-712a-4df7-c3c6-394ffd3ecfff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_df, val_df = train_test_split(one_hot_df, test_size=0.15)\n",
        "train_df.shape, val_df.shape"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((11537, 28), (2036, 28))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmFlcRB7pOTD",
        "colab_type": "code",
        "outputId": "38679c35-0092-4ebb-92b4-6d6bed6f4053",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        }
      },
      "source": [
        "train_old_df =pd.read_csv('/content/gdrive/My Drive/WRN_Extend/train_df_wrn2_widrn_acc_1577202722.csv') \n",
        "train_df = train_old_df\n",
        "val_old_df = pd.read_csv('/content/gdrive/My Drive/WRN_Extend/val_df_wrn2_widrn_acc_1577202722.csv')\n",
        "val_df=val_old_df\n",
        "print(train_df.shape, val_df.shape)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-60f10d6e2f11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_old_df\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive/My Drive/WRN_Extend/train_df_wrn2_widrn_acc_1577202722.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_old_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mval_old_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive/My Drive/WRN_Extend/val_df_wrn2_widrn_acc_1577202722.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mval_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_old_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    683\u001b[0m         )\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1135\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1136\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1917\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'/content/gdrive/My Drive/WRN_Extend/train_df_wrn2_widrn_acc_1577202722.csv' does not exist: b'/content/gdrive/My Drive/WRN_Extend/train_df_wrn2_widrn_acc_1577202722.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dzg--2pv7G4M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIYrhzCAbWv0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from datetime import datetime\n",
        "def get_curr_time():\n",
        "    return int(datetime.utcnow().strftime(\"%s\"))\n",
        "\n",
        "model_name_itr = 'wrn2_rkg_fresh_wrn_wide_rjy2_gml2_'+str(get_curr_time())\n",
        "gdrive_home_path=\"/content/gdrive/My Drive/WRN_Extend/\"\n",
        "train_csv=gdrive_home_path+\"train_df_\"+model_name_itr+\".csv\"\n",
        "val_csv=gdrive_home_path+\"val_df_\"+model_name_itr+\".csv\"\n",
        "json_file=gdrive_home_path+\"json_\"+model_name_itr+\".json\"\n",
        "png_file=gdrive_home_path+\"png_\"+model_name_itr+\".png\"\n",
        "weights_file=gdrive_home_path+\"h5_\"+model_name_itr+\".h5\"\n",
        "\n",
        "print(\"Model-name:\",model_name_itr)\n",
        "print(train_csv,val_csv,json_file,png_file,weights_file)\n",
        "train_df.to_csv(train_csv, index=False)\n",
        "val_df.to_csv(val_csv, index=False)\n",
        "\n",
        "# /content/gdrive/My Drive/train_df_wrn2_widrn_acc_1577202722.csv \n",
        "# /content/gdrive/My Drive/val_df_wrn2_widrn_acc_1577202722.csv \n",
        "# /content/gdrive/My Drive/json_wrn2_widrn_acc_1577202722.json\n",
        "#  /content/gdrive/My Drive/png_wrn2_widrn_acc_1577202722.png \n",
        "#  /content/gdrive/My Drive/h5_wrn2_widrn_acc_1577202722.h5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1L33i_QqLRe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Model-name:\",model_name_itr)\n",
        "for var_name in [train_csv,val_csv,json_file,png_file,weights_file]:\n",
        "  print(var_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVdL4WRuG2BC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output_weights = [\"gender_output\", \"imagequality_ouput\",\"age_output\", \"weight_output\", \"bag_output\", \"footwear_output\",\"emotion_output\", \"pose_output\"]\n",
        "col_splits = [_gender_cols_, _imagequality_cols_,_age_cols_, _weight_cols_, _carryingbag_cols_, _footwear_cols_, _emotion_cols_, _bodypose_cols_]\n",
        "def get_dist(train_df,equalize_classes=False):\n",
        "    loss_weights = {}\n",
        "    index=0\n",
        "    for selector_column in col_splits:\n",
        "        print(selector_column)\n",
        "        count = []\n",
        "        percentile = []\n",
        "        for age_split in selector_column:\n",
        "            count.append( train_df[selector_column][train_df[age_split] == 1].shape[0])\n",
        "        #print(count, np.round((count/11537.0)*100.0, 2))\n",
        "\n",
        "        max_val = np.max(count)\n",
        "        total_count = np.float32(train_df.shape[0])\n",
        "        #print(count, )\n",
        "        count_weights= [np.round(max_val/current_val,3) for current_val in count]\n",
        "        print(count_weights)\n",
        "        print(np.round((np.asarray(count)/total_count)*100.0, 2))\n",
        "\n",
        "        #print(\"Top Class:\",selector_column[np.argmax(count)],\"Max Count\",np.max(count))\n",
        "        #print(\"Bottom Class:\",selector_column[np.argmin(count)])\n",
        "        #weights_dist = dict(zip(selector_column, count_weights))\n",
        "        \n",
        "        #print(weights_dist)\n",
        "        weights_vals_dist={}\n",
        "        index_val=0\n",
        "        for y in range(len(count_weights)):\n",
        "            weights_vals_dist[y]=count_weights[y]\n",
        "            print(weights_vals_dist[y],y)\n",
        "            #index_val+=index_val\n",
        "            \n",
        "            #loss_weights[output_weights[index]]={x,y}\n",
        "        loss_weights[output_weights[index]]=weights_vals_dist\n",
        "        #if equalize_classes == True:\n",
        "        #    expanded_df = equalize_classwise_dist(train_df, selector_column, count)\n",
        "        #    train_df = train_df.append(expanded_df, ignore_index=True)\n",
        "        index+=1\n",
        "    #print(loss_weights)\n",
        "    return train_df,loss_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcR5l-K0HESp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "_,loss_weights_train=get_dist(train_df, equalize_classes=False)\n",
        "loss_weights_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5m15DLyF2ot",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSZIV48it799",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def random_crop(img, random_crop_size):\n",
        "    # Note: image_data_format is 'channel_last'\n",
        "    assert img.shape[2] == 3\n",
        "    height, width = img.shape[0], img.shape[1]\n",
        "    dy, dx = random_crop_size\n",
        "    x = np.random.randint(0, width - dx + 1)\n",
        "    y = np.random.randint(0, height - dy + 1)\n",
        "    return img[y:(y+dy), x:(x+dx), :]\n",
        "\n",
        "\n",
        "def crop_generator(batches, crop_length):\n",
        "    \"\"\"Take as input a Keras ImageGen (Iterator) and generate random\n",
        "    crops from the image batches generated by the original iterator.\n",
        "    \"\"\"\n",
        "    while True:\n",
        "        batch_x, batch_y = next(iter(batches))\n",
        "        batch_crops = np.zeros((batch_x.shape[0], crop_length, crop_length, 3))\n",
        "        for i in range(batch_x.shape[0]):\n",
        "            batch_crops[i] = random_crop(batch_x[i], (crop_length, crop_length))\n",
        "        yield (batch_crops, batch_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hco1q4yMxji3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "def get_random_eraser(p=0.5, s_l=0.02, s_h=0.4, r_1=0.3, r_2=1/0.3, v_l=0, v_h=255, pixel_level=False):\n",
        "    def eraser(input_img):\n",
        "        img_h, img_w, img_c = input_img.shape\n",
        "        p_1 = np.random.rand()\n",
        "        if p_1 > p:\n",
        "            return input_img\n",
        "        while True:\n",
        "            s = np.random.uniform(s_l, s_h) * img_h * img_w\n",
        "            r = np.random.uniform(r_1, r_2)\n",
        "            w = int(np.sqrt(s / r))\n",
        "            h = int(np.sqrt(s * r))\n",
        "            left = np.random.randint(0, img_w)\n",
        "            top = np.random.randint(0, img_h)\n",
        "            if left + w <= img_w and top + h <= img_h:\n",
        "                break\n",
        "        if pixel_level:\n",
        "            c = np.random.uniform(v_l, v_h, (h, w, img_c))\n",
        "        else:\n",
        "            c = np.random.uniform(v_l, v_h)\n",
        "        input_img[top:top + h, left:left + w, :] = c\n",
        "        return input_img\n",
        "    return eraser"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTiOi5tVBnhS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create train and validation data generators\n",
        "BATCH_SIZE=16\n",
        "aug_gen = ImageDataGenerator(horizontal_flip=True, \n",
        "                             vertical_flip=False,\n",
        "                             rotation_range=5,\n",
        "                             width_shift_range=0.1,\n",
        "                             height_shift_range=0.1,\n",
        "                             zoom_range=[0.5,2.5],\n",
        "                             shear_range=0.2,\n",
        "                             #zca_whitening=True,\n",
        "                             brightness_range=[0.5,2.5],\n",
        "                             #preprocessing_function=get_random_eraser(v_l=0, v_h=1, pixel_level=False)\n",
        "                             )\n",
        "\n",
        "train_gen = PersonDataGenerator(train_df, batch_size=BATCH_SIZE,normalize=True,aug_flow=aug_gen)\n",
        "valid_gen = PersonDataGenerator(val_df, batch_size=BATCH_SIZE, shuffle=False,normalize=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2y9D1I-udt_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CROP_LENGTH=64\n",
        "train_crops = crop_generator(train_gen, CROP_LENGTH)\n",
        "valid_crops = crop_generator(valid_gen, CROP_LENGTH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BbkJCyZXZx-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_gen[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_zwfsq5qPZ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_image_batch(data_df, batch_size=32, shuffle=True,normalize=True, selected_field='age_output'):\n",
        "    new_batch = PersonDataGenerator(data_df, batch_size,shuffle, normalize)\n",
        "    images, targets = next(iter(new_batch))\n",
        "    num_units = { k.split(\"_output\")[0]:v.shape[1] for k, v in targets.items()}\n",
        "    labels = np.asarray([ np.argmax(targets['age_output'][pos]) for pos in range(len(targets['age_output'])) ])\n",
        "    return images,labels, targets, len(images)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_IZu45hUsPkL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "images, y_train, targets, len_train = get_image_batch(train_df, batch_size=32,normalize=True, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usX4GK8btqZp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "images_test, y_test, targets_test, len_test = get_image_batch(val_df, batch_size=32,normalize=True, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cb1XGwFvwmK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_mean_std_for_batch(datagen_process):\n",
        "    image_val,target = next(iter(datagen_process))\n",
        "    print(image_val.shape)\n",
        "    print(np.mean(image_val.round(2), axis=(0,1,2)),np.std(image_val.round(2), axis=(0,1,2)) )\n",
        "    #print(np.mean(image_val.round(2), axis=(0,1,2)),np.std(image_val.round(2), axis=(0,1,2)) )  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfyX3wQN8ZQz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "images, targets = next(iter(train_gen))\n",
        "num_units = { k.split(\"_output\")[0]:v.shape[1] for k, v in targets.items()}\n",
        "num_units\n",
        "\n",
        "images_test, targets_test = next(iter(valid_gen))\n",
        "\n",
        "print(num_units)\n",
        "#print(np.mean(images.round(2), axis=(0,1,2)),np.std(images.round(2), axis=(0,1,2)) )\n",
        "#print(np.mean(images_test.round(2), axis=(0,1,2)),np.std(images_test.round(2), axis=(0,1,2)) )\n",
        "\n",
        "print_mean_std_for_batch(train_gen)\n",
        "print_mean_std_for_batch(valid_gen)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uwZZR7lpd54",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "images_test, targets_test = next(iter(valid_gen))\n",
        "num_units_test = { k.split(\"_output\")[0]:v.shape[1] for k, v in targets_test.items()}\n",
        "num_units_test\n",
        "#images.shape\n",
        "y_test  = np.asarray([ np.argmax(targets_test['age_output'][pos]) for pos in range(len(targets_test['age_output'])) ]) ## Taking the argmax to select the correct class\n",
        "cv2_imshow(cv2.resize(images_test[0], (images_test[0].shape[1], images_test[0].shape[0])))\n",
        "y_test.shape\n",
        "y_test[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjP5xE2CcM_d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def display_single_image(image):\n",
        "    cv2_imshow(cv2.resize(image, (image.shape[1], image.shape[0])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TKbaDy5pO_w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len_train, len_test = len(images), len(images_test)\n",
        "\n",
        "# train_mean = np.mean(images, axis=(0,1,2))\n",
        "# train_std = np.std(images, axis=(0,1,2))\n",
        "\n",
        "# normalize = lambda x: ((x - train_mean) / train_std).astype('float32') # todo: check here\n",
        "# #pad4 = lambda x: np.pad(x, [(0, 0), (4, 4), (4, 4), (0, 0)], mode='reflect')\n",
        "\n",
        "# images_norm = normalize(images)\n",
        "# images_test_norm = normalize(images_test)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FdbGK4nb8gI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#cv2_imshow(cv2.resize(images_norm[0], (images_norm[0].shape[1], images_norm[0].shape[0])))\n",
        "#images_norm.shape\n",
        "#images_test[0].shape\n",
        "display_single_image(images[10])\n",
        "#display_single_image(images_test_norm[10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LB6-3atp3iAx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.contrib.eager as tfe\n",
        "import time,math\n",
        "############# Weights initializer #################\n",
        "def init_pytorch(shape, dtype=tf.float32, partition_info=None):\n",
        "  fan = np.prod(shape[:-1])\n",
        "  bound = 1 / math.sqrt(fan)\n",
        "  return tf.random.uniform(shape, minval=-bound, maxval=bound, dtype=dtype)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMcoEEmPApbG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import AveragePooling2D, Input, Flatten\n",
        "from keras.optimizers import Adam,SGD\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.regularizers import l2\n",
        "from keras import backend as K"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06AAPOdlNSKC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, Add, Activation, Dropout, Flatten, Dense\n",
        "from keras.layers.convolutional import Convolution2D, MaxPooling2D, AveragePooling2D\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.regularizers import l2\n",
        "from keras import backend as K"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSSxGw9KOLfK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#weight_decay = 0.0005\n",
        "weight_decay = 0.001\n",
        "\n",
        "def initial_conv(input):\n",
        "    x = Conv2D(16, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                      W_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(input)\n",
        "\n",
        "    channel_axis = -1 #if K.image_data_format() == \"channels_first\" else -1\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(x)\n",
        "    x = Activation('relu')(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def expand_conv(init, base, k, strides=(1, 1)):\n",
        "    x = Conv2D(base * k, (3, 3), padding='same', strides=strides, kernel_initializer='he_normal',\n",
        "                      W_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(init)\n",
        "\n",
        "    channel_axis = -1 #if K.image_data_format() == \"channels_first\" else -1\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = Conv2D(base * k, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                      W_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(x)\n",
        "\n",
        "    skip = Conv2D(base * k, (1, 1), padding='same', strides=strides, kernel_initializer='he_normal',\n",
        "                      W_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(init)\n",
        "\n",
        "    m = Add()([x, skip])\n",
        "\n",
        "    return m\n",
        "\n",
        "\n",
        "def conv1_block(input, k=1, dropout=0.0):\n",
        "    init = input\n",
        "\n",
        "    channel_axis = -1# if K.image_data_format() == \"channels_first\" else -1\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(input)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(16 * k, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                      W_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(x)\n",
        "\n",
        "    if dropout > 0.0: x = Dropout(dropout)(x)\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(16 * k, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                      W_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(x)\n",
        "\n",
        "    m = Add()([init, x])\n",
        "    return m\n",
        "\n",
        "def conv2_block(input, k=1, dropout=0.0):\n",
        "    init = input\n",
        "\n",
        "    channel_axis = -1 #if K.image_dim_ordering() == \"th\" else -1\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(input)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(32 * k, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                      W_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(x)\n",
        "\n",
        "    if dropout > 0.0: x = Dropout(dropout)(x)\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(32 * k, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                      W_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(x)\n",
        "\n",
        "    m = Add()([init, x])\n",
        "    return m\n",
        "\n",
        "def conv3_block(input, k=1, dropout=0.0):\n",
        "    init = input\n",
        "\n",
        "    channel_axis = -1 #if K.image_dim_ordering() == \"th\" else -1\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(input)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(64 * k, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                      W_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(x)\n",
        "\n",
        "    if dropout > 0.0: x = Dropout(dropout)(x)\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(64 * k, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                      W_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(x)\n",
        "\n",
        "    m = Add()([init, x])\n",
        "    return m\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-0O2LzQOPzA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_wide_residual_network(input_dim, nb_classes=100, N=2, k=1, dropout=0.0, verbose=1,num_units_in=num_units):\n",
        "    \"\"\"\n",
        "    Creates a Wide Residual Network with specified parameters\n",
        "    :param input: Input Keras object\n",
        "    :param nb_classes: Number of output classes\n",
        "    :param N: Depth of the network. Compute N = (n - 4) / 6.\n",
        "              Example : For a depth of 16, n = 16, N = (16 - 4) / 6 = 2\n",
        "              Example2: For a depth of 28, n = 28, N = (28 - 4) / 6 = 4\n",
        "              Example3: For a depth of 40, n = 40, N = (40 - 4) / 6 = 6\n",
        "    :param k: Width of the network.\n",
        "    :param dropout: Adds dropout if value is greater than 0.0\n",
        "    :param verbose: Debug info to describe created WRN\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    channel_axis = -1 #if K.image_data_format() == \"channels_first\" else -1\n",
        "\n",
        "    ip = Input(shape=input_dim)\n",
        "\n",
        "    x = initial_conv(ip)\n",
        "    nb_conv = 4\n",
        "\n",
        "    x = expand_conv(x, 16, k)\n",
        "    nb_conv += 2\n",
        "\n",
        "    for i in range(N - 1):\n",
        "        x = conv1_block(x, k, dropout)\n",
        "        nb_conv += 2\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = expand_conv(x, 32, k, strides=(2, 2))\n",
        "    nb_conv += 2\n",
        "\n",
        "    for i in range(N - 1):\n",
        "        x = conv2_block(x, k, dropout)\n",
        "        nb_conv += 2\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = expand_conv(x, 64, k, strides=(2, 2))\n",
        "    nb_conv += 2\n",
        "\n",
        "    for i in range(N - 1):\n",
        "        x = conv3_block(x, k, dropout)\n",
        "        nb_conv += 2\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = AveragePooling2D((8, 8))(x)\n",
        "    x = Flatten()(x)\n",
        "\n",
        "    #x = Dense(nb_classes, W_regularizer=l2(weight_decay), activation='softmax')(x)\n",
        "    output_layers = [Dense(num_units[name], activation=\"softmax\", name=f\"{name}_output\")(x) for name in num_units_in.keys()]\n",
        "    model = Model(ip, output_layers)\n",
        "\n",
        "    #if verbose: print(\"Wide Residual Network-%d-%d created.\" % (nb_conv, k))\n",
        "    return model\n",
        "    #return x\n",
        "\n",
        "def add_final_layer(model,num_units_in):\n",
        "    output_vals = [Dense(num_units[name], activation=\"softmax\", name=f\"{name}_output\")(model) for name in num_units_in.keys()]\n",
        "    return output_vals\n",
        "\n",
        "def create_model(dropout=0.05):\n",
        "    from keras.utils import plot_model\n",
        "    from keras.layers import Input\n",
        "    from keras.models import Model\n",
        "\n",
        "    init = (224, 224, 3)\n",
        "\n",
        "    #wrn_28_10 = create_wide_residual_network(init, nb_classes=10, N=2, k=2, dropout=0.0)\n",
        "    wrn_28_10 = create_wide_residual_network(init, nb_classes=10, N=2, k=4, dropout=dropout)\n",
        "    return wrn_28_10\n",
        "    #output_layers = add_final_layer(wrn_28_10_backbone, num_units)\n",
        "    #model = Model(Input(shape=init), output_layers)\n",
        "    #wrn_28_10.summary()\n",
        "\n",
        "    #plot_model(wrn_28_10, \"WRN-16-2.png\", show_shapes=True, show_layer_names=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRJcOZ_VAQzA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.compat.v1.disable_eager_execution()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5leq1Jrxz3u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wrn_28_10.losses"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJhxE2F1HmKH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_weights_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfPG9C2eA1zn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# wrn_28_10.compile(\n",
        "#     optimizer=SGD(lr=0.5),\n",
        "#     loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "#     # loss_weights=loss_weights, \n",
        "#     metrics=[\"accuracy\"]\n",
        "# )\n",
        "wrn_28_10=create_model()\n",
        "wrn_28_10.compile(\n",
        "    optimizer=SGD(lr=0.049203925),\n",
        "    #,momentum=MOMENTUM, nesterov=True),\n",
        "    #optimizer=SGD(lr=0.01191919191919192),\n",
        "    loss=tf.keras.losses.CategoricalCrossentropy(),     \n",
        "    weighted_metrics=[\"accuracy\"]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfzXsa5sVZbE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Min: 12.520438 0.57330394\n",
        "#Min: 12.54081 0.586987\n",
        "#Min: 12.628042 0.18920188\n",
        "#del wrn_28_10\n",
        "#Min: 7.8906326 1.3513402\n",
        "def iterate_lr_finder(start_lr=0.1,end_lr=1):\n",
        "  \n",
        "    model=create_model()\n",
        "    loss_weights_compile = {'gender_output': 2, \n",
        "                        'image_quality_output': 2, \n",
        "                        'age_output': 4, \n",
        "                        'weight_output': 3, \n",
        "                        'bag_output': 3, \n",
        "                        'pose_output': 3, \n",
        "                        'footwear_output': 2, \n",
        "                        'emotion_output': 4}\n",
        "    model.compile(\n",
        "        optimizer=SGD(lr=0.049203925),\n",
        "        #,momentum=MOMENTUM, nesterov=True),\n",
        "        #optimizer=SGD(lr=0.01191919191919192),\n",
        "        loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "        loss_weights=loss_weights_compile,  \n",
        "        #metrics=[\"accuracy\"]\n",
        "        weighted_metrics=[\"accuracy\"]\n",
        "    )\n",
        "\n",
        "\n",
        "    lr_finder = LRFinder_new(model)\n",
        "    lr_finder.find_generator(train_gen,start_lr=start_lr, end_lr=end_lr,epochs=10,steps_per_epoch=20)#,class_weight=loss_weights_train)\n",
        "    #lr_finder.find_generator(train_gen,start_lr=0.0001, end_lr=1,epochs=10,steps_per_epoch=20)\n",
        "\n",
        "    print(\"#start_lr\",start_lr,\"end_lr\",end_lr)\n",
        "    print(\"#Max:\", np.max(lr_finder.losses),lr_finder.lrs[np.argmax(lr_finder.losses)])\n",
        "    print(\"#Min:\", np.min(lr_finder.losses), lr_finder.lrs[np.argmin(lr_finder.losses)])\n",
        "    \n",
        "    del model\n",
        "    return lr_finder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ya1WsABSbV-8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr_1 = iterate_lr_finder(start_lr=0.1,end_lr=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGkNg0KScLpv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr_1.plot_loss(n_skip_end=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBgetG9scRDx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr_2 = iterate_lr_finder(start_lr=0.001,end_lr=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_Fbgpt3cuaD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr_3 = iterate_lr_finder(start_lr=0.0001,end_lr=0.001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyzEOah2Z_mr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(lr_finder.get_best_lr(1,1,1))\n",
        "print(lr_finder.get_best_lr(10,1,1))\n",
        "print(lr_finder.get_best_lr(20,1,1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ND4LUezqnx96",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lrs_to_losses = pd.DataFrame({'lrs':lr_finder.lrs, 'losses':lr_finder.losses})\n",
        "lrs_to_losses.loc[lrs_to_losses.shape[0]-1]\n",
        "sma_10_lr = lr_finder.get_best_lr(10,1,1)\n",
        "lrs_to_losses.loc[lrs_to_losses[lrs_to_losses['lrs'].between(sma_10_lr*0.1, sma_10_lr) ]['losses'].idxmin()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PasUsGbpr80",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lrs_to_losses"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gw6LOmnuLSdd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.min(lr_finder.losses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMw7fR8XQK1l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Max:\", np.max(lr_finder.losses),lr_finder.lrs[np.argmax(lr_finder.losses)])\n",
        "print(\"Min:\", np.min(lr_finder.losses), lr_finder.lrs[np.argmin(lr_finder.losses)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wj2TfaqULMg5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr_finder.plot_loss(n_skip_end=1)\n",
        "#lr_finder.plot_loss(n_skip_end=1,x_scale='linear')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rt5qv_Ndp8Q-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for (k,v) in num_units.items():\n",
        "    print(k,v)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10qS64vScOD5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#wrn_28_10=create_model()\n",
        "from keras.models import load_model\n",
        "# import the necessary packages\n",
        "from keras.callbacks import BaseLogger\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "\n",
        "# class LossHistory(keras.callbacks.Callback):\n",
        "# \tdef on_train_begin(self, logs={}):\n",
        "# \t\tprint(\"Clearing saved content on training start\")\n",
        "# \t\tself.losses = []\n",
        "# \t\tself.best = np.Inf\n",
        "\n",
        "class TrainingMonitor(BaseLogger):\n",
        "\tdef __init__(self, figPath, jsonPath=None, startAt=0, backup_hist=True):\n",
        "\t\t# store the output path for the figure, the path to the JSON\n",
        "\t\t# serialized file, and the starting epoch\n",
        "\t\tsuper(TrainingMonitor, self).__init__()\n",
        "\t\tself.figPath = figPath\n",
        "\t\tself.jsonPath = jsonPath\n",
        "\t\tself.startAt = startAt\n",
        "\t\tself.backup_hist = backup_hist\n",
        "\t\tprint(\"JSON path:\",self.jsonPath)\n",
        "\n",
        "\tdef on_train_begin(self, logs={}):\n",
        "\t\t# initialize the history dictionary\n",
        "\t\tself.H = {}\n",
        "\t\t#self.losses = []\n",
        "\n",
        "\t\t# if the JSON history path exists, load the training history\n",
        "\t\tif self.jsonPath is not None:\n",
        "\t\t\tif os.path.exists(self.jsonPath) and (self.backup_hist == True):\n",
        "\t\t\t\t#self.H = json.loads(open(self.jsonPath).read())\n",
        "\t\t\t\tbackup_file_name=self.jsonPath+str(get_curr_time())+\"_backup\"\n",
        "\t\t\t\tprint(\"Backing up history file:\",self.jsonPath,\" to:\",backup_file_name)\n",
        "\t\t\t\tos.rename(self.jsonPath,backup_file_name) \n",
        "\n",
        "\t\t\t\t# # check to see if a starting epoch was supplied\n",
        "\t\t\t\t# if self.startAt > 0:\n",
        "\t\t\t\t# \t# loop over the entries in the history log and\n",
        "\t\t\t\t# \t# trim any entries that are past the starting\n",
        "\t\t\t\t# \t# epoch\n",
        "\t\t\t\t# \tfor k in self.H.keys():\n",
        "\t\t\t\t# \t\tself.H[k] = self.H[k][:self.startAt]\n",
        "\n",
        "\tdef on_epoch_end(self, epoch, logs={}):\n",
        "\t\t# loop over the logs and update the loss, accuracy, etc.\n",
        "\t\t# for the entire training process\n",
        "\t\tfor (k, v) in logs.items():\n",
        "\t\t\tl = self.H.get(k, [])\n",
        "\t\t\tl.append(float(v))\n",
        "\t\t\tself.H[k] = l\n",
        "\n",
        "\t\t# check to see if the training history should be serialized\n",
        "\t\t# to file\n",
        "\t\tif self.jsonPath is not None:\n",
        "\t\t\tf = open(self.jsonPath, \"w\")\n",
        "\t\t\tf.write(json.dumps(self.H))\n",
        "\t\t\tf.close()\n",
        "\tdef on_train_end(self, logs={}):\n",
        "\t\tbackup_file_name=self.jsonPath+str(get_curr_time())+\"_backup\"\n",
        "\t\t#print(\"Backing up history file:\",self.jsonPath,\" to:\",backup_file_name)\n",
        "\t\tos.rename(self.jsonPath,backup_file_name) \n",
        "\t\tprint(\"Current JSON PATH:\",self.jsonPath)\n",
        "\t\tprint(\"Final JSON PATH:\",backup_file_name)\t\t\n",
        "\n",
        "import os\n",
        "plotPath = png_file\n",
        "jsonPath = json_file\n",
        "print(plotPath,jsonPath)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oC-pZQP-WaNO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from datetime import datetime\n",
        "# Prepare model model saving directory.\n",
        "import os\n",
        "save_dir = os.path.join('/content/gdrive/', 'My Drive')\n",
        "\n",
        "model_name = 'assignment5_%s_model.{epoch:03d}.h5' % (model_name_itr+\"rd2\")\n",
        "print(model_name)\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "filepath = os.path.join(save_dir, model_name)\n",
        "\n",
        "# Prepare callbacks for model saving and for learning rate adjustment.\n",
        "# checkpoint = ModelCheckpoint(filepath=filepath,\n",
        "#                              monitor='val_loss',\n",
        "#                              verbose=1,\n",
        "# #                              save_best_only=True)\n",
        "\n",
        "# checkpoint = ModelCheckpoint(filepath=filepath,\n",
        "#                              monitor='val_loss',\n",
        "#                              verbose=1,\n",
        "#                              save_best_only=True,mode='min')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1HpOmnIWPtl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS=50\n",
        "#LEARNING_RATE=1.3513402*0.1\n",
        "#LEARNING_RATE=0.18883973*0.1\n",
        "#LEARNING_RATE=0.5037587*0.1\n",
        "LEARNING_RATE=0.020183668*0.15\n",
        "STEPS_PER_EPOCH=100\n",
        "test_y = np.linspace(0,EPOCHS,EPOCHS)\n",
        "x=[0, (EPOCHS+1)//5, EPOCHS]\n",
        "y=[LEARNING_RATE*0.01, LEARNING_RATE, LEARNING_RATE*0.0001]\n",
        "interp_lr = np.interp(test_y, x, y)\n",
        "def one_lr_schedule(epoch):\n",
        "    # if(epoch <= 15):\n",
        "    #     print(\"lr:\",interp_lr[epoch+84],epoch)\n",
        "    #     return interp_lr[epoch+84]\n",
        "    print(\"lr:\",interp_lr[epoch],epoch)\n",
        "    return interp_lr[epoch]\n",
        "#interp_values = np.interp(, [0, (EPOCHS+1)//5, EPOCHS], [0, LEARNING_RATE, 0])\n",
        "lr_scheduler = LearningRateScheduler(one_lr_schedule)\n",
        "# callbacks = [checkpoint, lr_scheduler,TrainingMonitor(figPath=plotPath,\n",
        "#                                                       jsonPath=jsonPath,startAt=2)]\n",
        "callbacks = [checkpoint, clr,TrainingMonitor(figPath=plotPath,\n",
        "                                                      jsonPath=jsonPath,startAt=0)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHx46QbROE-3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "save_dir = os.path.join('/content/gdrive/', 'My Drive/WRN_Extend')\n",
        "\n",
        "\n",
        "def generate_new_callbacks(steps_per_epoch=50,\n",
        "                           epoch_count=50,\n",
        "                           min_lr=0.00001, \n",
        "                           max_lr=0.1,\n",
        "                           patience=25,\n",
        "                           check_point=True,\n",
        "                           clr_mode='triangular',\n",
        "                           clr_multiplier=4):\n",
        "    model_name = 'assignment5_%s_model.{epoch:03d}.h5' % (model_name_itr+\"_\"+str(get_curr_time()))\n",
        "    print(model_name)\n",
        "    if not os.path.isdir(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "    filepath = os.path.join(save_dir, model_name)\n",
        "    local_clr = CyclicLR(base_lr=min_lr,\n",
        "                            max_lr=max_lr,\n",
        "                            step_size=steps_per_epoch*clr_multiplier,\n",
        "                            mode=clr_mode)\n",
        "\n",
        "    training_mon = TrainingMonitor(figPath=plotPath,\n",
        "                                   jsonPath=jsonPath,\n",
        "                                   startAt=0)\n",
        "    ########## Introduced after 2x100 Epochs\n",
        "    early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
        "                                                    patience=patience, \n",
        "                                                    restore_best_weights=True)\n",
        "    print(\"Returning new callback array with steps_per_epoch=\",steps_per_epoch,\n",
        "          \"min_lr=\",min_lr,\n",
        "          \"max_lr=\",max_lr,\n",
        "          \"epoch_count=\",epoch_count,\n",
        "          \"patience=\",patience\n",
        "          )\n",
        "    callback_array = [local_clr, early_stop,training_mon]\n",
        "    if(check_point == True):\n",
        "        checkpoint = ModelCheckpoint(filepath=filepath,\n",
        "                                monitor='val_loss',\n",
        "                                verbose=1,\n",
        "                                save_best_only=True,\n",
        "                                mode='min')\n",
        "        callback_array.append(checkpoint)\n",
        "    \n",
        "    return callback_array"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09u1PORzY1LS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install git+https://www.github.com/keras-team/keras-contrib.git\n",
        "from keras_contrib.callbacks import CyclicLR\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWUomQO7jBhQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_weights_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fG49QJrPglub",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_model_iterations(model,\n",
        "                         re_compile=True,\n",
        "                         epoch_count=50, \n",
        "                         steps_per_epoch=50, \n",
        "                         min_lr=LEARNING_RATE*0.01, \n",
        "                         max_lr=LEARNING_RATE,\n",
        "                         loss_weights_compile={},\n",
        "                         loss_weights_train={}                         \n",
        "                         ):\n",
        "\n",
        "\n",
        "    if re_compile == True:\n",
        "        model.compile(\n",
        "            #optimizer=SGD(lr=1.3513402*0.1),\n",
        "            optimizer=SGD(lr=min_lr),\n",
        "            loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "            loss_weights=loss_weights_compile,\n",
        "            #weighted_metrics=[\"accuracy\"]\n",
        "            metrics=[\"accuracy\"]\n",
        "        )\n",
        "    model.fit_generator(\n",
        "        generator=train_gen,\n",
        "        validation_data=valid_gen,\n",
        "        use_multiprocessing=True,\n",
        "        workers=4, \n",
        "        epochs=1,\n",
        "        verbose=1,\n",
        "        class_weight=loss_weights_train,\n",
        "        steps_per_epoch=steps_per_epoch,\n",
        "        callbacks=generate_new_callbacks(steps_per_epoch=steps_per_epoch, min_lrm=in_lr, max_lr=max_lr, epoch_count=epoch_count)\n",
        "    )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzipUJFJjYRZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wrn_28_10=create_model()\n",
        "run_model_iterations(wrn_28_10,\n",
        "                    re_compile=True,\n",
        "                    epoch_count=1, \n",
        "                    steps_per_epoch=1, \n",
        "                    min_lr=LEARNING_RATE*0.01, \n",
        "                    max_lr=LEARNING_RATE,\n",
        "                    loss_weights_compile={},\n",
        "                    loss_weights_train={})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ay1_bpi-aW_I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#LEARNING_RATE=1.3513402*0.1\n",
        "#del wrn_28_10\n",
        "#wrn_28_10=create_model()\n",
        "wrn_28_10=create_model()\n",
        "LEARNING_RATE=0.2511886*0.1\n",
        "STEPS_PER_EPOCH=30\n",
        "EPOCHS=100\n",
        "loss_weights_compile = {'gender_output': 2, \n",
        "                        'image_quality_output': 2, \n",
        "                        'age_output': 4, \n",
        "                        'weight_output': 3, \n",
        "                        'bag_output': 3, \n",
        "                        'pose_output': 3, \n",
        "                        'footwear_output': 2, \n",
        "                        'emotion_output': 4}\n",
        "\n",
        "wrn_28_10.compile(\n",
        "     #optimizer=SGD(lr=1.3513402*0.1),\n",
        "     optimizer=SGD(lr=0.0001),\n",
        "     loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "     loss_weights=loss_weights_compile,\n",
        "     #weighted_metrics=[\"accuracy\"]\n",
        "     metrics=[\"accuracy\"]\n",
        ")\n",
        "callbacks = generate_new_callbacks(steps_per_epoch=STEPS_PER_EPOCH, \n",
        "                                     epoch_count=EPOCHS,\n",
        "                                     min_lr=LEARNING_RATE*0.01, \n",
        "                                     max_lr=LEARNING_RATE)\n",
        "#print(callbacks)\n",
        "wrn_28_10.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=4, \n",
        "    epochs=EPOCHS,\n",
        "    verbose=1,\n",
        "    class_weight=loss_weights_train,\n",
        "    steps_per_epoch=STEPS_PER_EPOCH,\n",
        "    callbacks=callbacks\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FvlpzZyyMUH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"End of EPOCHS=\",EPOCHS,\" STEPS_PER_EPOCH=\",STEPS_PER_EPOCH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjPUfY_NmI35",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#LEARNING_RATE=1.3513402*0.1\n",
        "#del wrn_28_10\n",
        "#wrn_28_10=create_model()\n",
        "wrn_28_10=create_model()\n",
        "LEARNING_RATE=0.2511886*0.1\n",
        "STEPS_PER_EPOCH=100\n",
        "EPOCHS=100\n",
        "# loss_weights_compile = {'gender_output': 2, \n",
        "#                         'image_quality_output': 2, \n",
        "#                         'age_output': 4, \n",
        "#                         'weight_output': 3, \n",
        "#                         'bag_output': 3, \n",
        "#                         'pose_output': 3, \n",
        "#                         'footwear_output': 2, \n",
        "#                         'emotion_output': 4}\n",
        "\n",
        "\n",
        "\n",
        "wrn_28_10.load_weights('/content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.097.h5')\n",
        "wrn_28_10.compile(\n",
        "     #optimizer=SGD(lr=1.3513402*0.1),\n",
        "     optimizer=SGD(lr=0.0001),\n",
        "     loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "     loss_weights=loss_weights_compile,\n",
        "     #weighted_metrics=[\"accuracy\"]\n",
        "     metrics=[\"accuracy\"]\n",
        ")\n",
        "callbacks = generate_new_callbacks(steps_per_epoch=STEPS_PER_EPOCH, \n",
        "                                     epoch_count=EPOCHS,\n",
        "                                     min_lr=LEARNING_RATE*0.01, \n",
        "                                     max_lr=LEARNING_RATE)\n",
        "#print(callbacks)\n",
        "wrn_28_10.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=4, \n",
        "    epochs=EPOCHS,\n",
        "    verbose=1,\n",
        "    class_weight=loss_weights_train,\n",
        "    steps_per_epoch=STEPS_PER_EPOCH,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "print(\"End of EPOCHS=\",EPOCHS,\" STEPS_PER_EPOCH=\",STEPS_PER_EPOCH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dls1akPMJKF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"End of EPOCHS=\",EPOCHS,\" STEPS_PER_EPOCH=\",STEPS_PER_EPOCH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g11dXe5JmI1I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#LEARNING_RATE=1.3513402*0.1\n",
        "#del wrn_28_10\n",
        "#wrn_28_10=create_model()\n",
        "STEPS_PER_EPOCH=200\n",
        "EPOCHS=100\n",
        "\n",
        "#wrn_28_10=create_model()\n",
        "wrn_28_10.load_weights('/content/gdrive/My Drive/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.097.h5')\n",
        "loss_weights_compile = {'gender_output': 2, \n",
        "                        'image_quality_output': 2, \n",
        "                        'age_output': 4, \n",
        "                        'weight_output': 3, \n",
        "                        'bag_output': 3, \n",
        "                        'pose_output': 3, \n",
        "                        'footwear_output': 2, \n",
        "                        'emotion_output': 4}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "wrn_28_10.compile(\n",
        "     #optimizer=SGD(lr=1.3513402*0.1),\n",
        "     optimizer=SGD(lr=0.0001),\n",
        "     loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "     loss_weights=loss_weights_compile,\n",
        "     #weighted_metrics=[\"accuracy\"]\n",
        "     metrics=[\"accuracy\"]\n",
        ")\n",
        "callbacks = generate_new_callbacks(steps_per_epoch=STEPS_PER_EPOCH, \n",
        "                                     epoch_count=EPOCHS,\n",
        "                                     min_lr=LEARNING_RATE*0.01, \n",
        "                                     max_lr=LEARNING_RATE)\n",
        "#print(callbacks)\n",
        "wrn_28_10.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=4, \n",
        "    epochs=EPOCHS,\n",
        "    verbose=1,\n",
        "    class_weight=loss_weights_train,\n",
        "    steps_per_epoch=STEPS_PER_EPOCH,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "print(\"End of EPOCHS=\",EPOCHS,\" STEPS_PER_EPOCH=\",STEPS_PER_EPOCH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5umiyWoNL91y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#LEARNING_RATE=1.3513402*0.1\n",
        "#del wrn_28_10\n",
        "#wrn_28_10=create_model()\n",
        "LEARNING_RATE=0.2511886*0.1\n",
        "STEPS_PER_EPOCH=train_df.shape[0]//BATCH_SIZE\n",
        "EPOCHS=30\n",
        "\n",
        "wrn_28_10=create_model()\n",
        "wrn_28_10.load_weights('/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_1577517949rd2_model.096.h5')\n",
        "loss_weights_compile = {'gender_output': 2, \n",
        "                        'image_quality_output': 2, \n",
        "                        'age_output': 4, \n",
        "                        'weight_output': 3, \n",
        "                        'bag_output': 3, \n",
        "                        'pose_output': 3, \n",
        "                        'footwear_output': 2, \n",
        "                        'emotion_output': 4}\n",
        "\n",
        "wrn_28_10.compile(\n",
        "     #optimizer=SGD(lr=1.3513402*0.1),\n",
        "     optimizer=SGD(lr=0.0001),\n",
        "     loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "     loss_weights=loss_weights_compile,\n",
        "     #weighted_metrics=[\"accuracy\"]\n",
        "     metrics=[\"accuracy\"]\n",
        ")\n",
        "callbacks = generate_new_callbacks(steps_per_epoch=STEPS_PER_EPOCH, \n",
        "                                     epoch_count=EPOCHS,\n",
        "                                     min_lr=LEARNING_RATE*0.01, \n",
        "                                     max_lr=LEARNING_RATE)\n",
        "#print(callbacks)\n",
        "wrn_28_10.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=4, \n",
        "    epochs=EPOCHS,\n",
        "    verbose=1,\n",
        "    class_weight=loss_weights_train,\n",
        "    steps_per_epoch=STEPS_PER_EPOCH,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "print(\"End of EPOCHS=\",EPOCHS,\" STEPS_PER_EPOCH=\",STEPS_PER_EPOCH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpTglbv2aaZ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wrn_28_10.save(\"/content/gdrive/My Drive/WRN_Extend/model_8233acc.h5py\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2cc5fxtr_v8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "aug_gen_array = [ ImageDataGenerator(horizontal_flip=True, \n",
        "                             vertical_flip=False,\n",
        "                             rotation_range=5,\n",
        "                             width_shift_range=0.1,\n",
        "                             height_shift_range=0.1,\n",
        "                             zoom_range=[0.5,2.5],\n",
        "                             shear_range=0.2,\n",
        "                             #zca_whitening=True,\n",
        "                             brightness_range=[0.5,2.5],\n",
        "                             #preprocessing_function=get_random_eraser(v_l=0, v_h=1, pixel_level=False)\n",
        "                             ),\n",
        "           ImageDataGenerator(horizontal_flip=True, \n",
        "                             vertical_flip=False,\n",
        "                             rotation_range=30,\n",
        "                             width_shift_range=0.1,\n",
        "                             height_shift_range=0.1,\n",
        "                             zoom_range=[0.5,2.5],\n",
        "                             shear_range=0.2,\n",
        "                             #zca_whitening=True,\n",
        "                             brightness_range=[0.5,2.5],\n",
        "                             #preprocessing_function=get_random_eraser(v_l=0, v_h=1, pixel_level=False)\n",
        "                             ),\n",
        "           ImageDataGenerator(horizontal_flip=True, \n",
        "                             vertical_flip=False,\n",
        "                             rotation_range=5,\n",
        "                             width_shift_range=0.5,\n",
        "                             height_shift_range=0.5,\n",
        "                             zoom_range=[0.5,2.5],\n",
        "                             shear_range=0.2,\n",
        "                             #zca_whitening=True,\n",
        "                             brightness_range=[0.5,2.5],\n",
        "                             #preprocessing_function=get_random_eraser(v_l=0, v_h=1, pixel_level=False)\n",
        "                             ),\n",
        "           ImageDataGenerator(horizontal_flip=True, \n",
        "                             vertical_flip=False,\n",
        "                             rotation_range=5,\n",
        "                             width_shift_range=0.1,\n",
        "                             height_shift_range=0.1,\n",
        "                             zoom_range=[0.5,3.5],\n",
        "                             shear_range=0.2,\n",
        "                             #zca_whitening=True,\n",
        "                             brightness_range=[0.5,2.5],\n",
        "                             #preprocessing_function=get_random_eraser(v_l=0, v_h=1, pixel_level=False)\n",
        "                             ),\n",
        "           ImageDataGenerator(horizontal_flip=True, \n",
        "                             vertical_flip=False,\n",
        "                             rotation_range=5,\n",
        "                             width_shift_range=0.1,\n",
        "                             height_shift_range=0.1,\n",
        "                             zoom_range=[0.5,3.5],\n",
        "                             shear_range=0.2,\n",
        "                             #zca_whitening=True,\n",
        "                             brightness_range=[0.5,2.5],\n",
        "                             #preprocessing_function=get_random_eraser(v_l=0, v_h=1, pixel_level=False)\n",
        "                             ),\n",
        "           ImageDataGenerator(horizontal_flip=True, \n",
        "                             vertical_flip=False,\n",
        "                             rotation_range=5,\n",
        "                             width_shift_range=0.1,\n",
        "                             height_shift_range=0.1,\n",
        "                             zoom_range=[0.5,2.5],\n",
        "                             shear_range=0.8,\n",
        "                             #zca_whitening=True,\n",
        "                             brightness_range=[0.5,4.5],\n",
        "                             #preprocessing_function=get_random_eraser(v_l=0, v_h=1, pixel_level=False)\n",
        "                             ),\n",
        "           ImageDataGenerator(horizontal_flip=True, \n",
        "                             vertical_flip=False,\n",
        "                             rotation_range=5,\n",
        "                             width_shift_range=0.1,\n",
        "                             height_shift_range=0.1,\n",
        "                             zoom_range=[0.5,2.5],\n",
        "                             shear_range=0.2,\n",
        "                             #zca_whitening=True,\n",
        "                             brightness_range=[0.5,2.5],\n",
        "                             preprocessing_function=get_random_eraser(v_l=0, v_h=1, pixel_level=False)\n",
        "                             )\n",
        "]\n",
        "           "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jzs8k2MswOUC",
        "colab": {}
      },
      "source": [
        "#LEARNING_RATE=1.3513402*0.1\n",
        "#del wrn_28_10\n",
        "#wrn_28_10=create_model()\n",
        "LEARNING_RATE=0.2511886*0.1\n",
        "STEPS_PER_EPOCH=30\n",
        "EPOCHS=50\n",
        "\n",
        "#wrn_28_10=create_model()\n",
        "# aug_gen = ImageDataGenerator(horizontal_flip=True, \n",
        "#                              vertical_flip=False,\n",
        "#                              rotation_range=5,\n",
        "#                              width_shift_range=0.1,\n",
        "#                              height_shift_range=0.1,\n",
        "#                              zoom_range=[0.5,2.5],\n",
        "#                              shear_range=0.2,\n",
        "#                              #zca_whitening=True,\n",
        "#                              brightness_range=[0.5,2.5],\n",
        "#                              #preprocessing_function=get_random_eraser(v_l=0, v_h=1, pixel_level=False)\n",
        "#                              )\n",
        "from keras.models import load_model\n",
        "for aug_gen_count in range(len(aug_gen_array)):\n",
        "    print(\"Executing augmentation\",aug_gen_count)\n",
        "    new_model = create_model()\n",
        "    new_model.load_weights('/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_1577543864_1577544293_model.022.h5')\n",
        "    \n",
        "    train_gen = PersonDataGenerator(train_df, \n",
        "                                    batch_size=BATCH_SIZE,\n",
        "                                    normalize=True,\n",
        "                                    aug_flow=aug_gen_array[aug_gen_count])\n",
        "    valid_gen = PersonDataGenerator(val_df, \n",
        "                                    batch_size=BATCH_SIZE, \n",
        "                                    shuffle=False,normalize=True)\n",
        "\n",
        "    #wrn_28_10.load_weights()\n",
        "\n",
        "    loss_weights_compile = {'gender_output': 2, \n",
        "                            'image_quality_output': 2, \n",
        "                            'age_output': 4, \n",
        "                            'weight_output': 3, \n",
        "                            'bag_output': 3, \n",
        "                            'pose_output': 3, \n",
        "                            'footwear_output': 2, \n",
        "                            'emotion_output': 4}\n",
        "\n",
        "    new_model.compile(\n",
        "        #optimizer=SGD(lr=1.3513402*0.1),\n",
        "        optimizer=SGD(lr=0.0001),\n",
        "        loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "        loss_weights=loss_weights_compile,\n",
        "        #weighted_metrics=[\"accuracy\"]\n",
        "        metrics=[\"accuracy\"]\n",
        "    )\n",
        "    callbacks = generate_new_callbacks(steps_per_epoch=STEPS_PER_EPOCH, \n",
        "                                        epoch_count=EPOCHS,\n",
        "                                        min_lr=LEARNING_RATE*0.01, \n",
        "                                        max_lr=LEARNING_RATE,\n",
        "                                        patience=10, check_point=False)\n",
        "    #print(callbacks)\n",
        "    new_model.fit_generator(\n",
        "        generator=train_gen,\n",
        "        validation_data=valid_gen,\n",
        "        use_multiprocessing=True,\n",
        "        workers=4, \n",
        "        epochs=EPOCHS,\n",
        "        verbose=1,\n",
        "        class_weight=loss_weights_train,\n",
        "        steps_per_epoch=STEPS_PER_EPOCH,\n",
        "        callbacks=callbacks\n",
        "    )\n",
        "    del new_model\n",
        "    print(\"End of EPOCHS=\",EPOCHS,\" STEPS_PER_EPOCH=\",STEPS_PER_EPOCH)\n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfKT__bEKI_L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###################### Couldn't find a good augmentation strategy hence sticking with defaults ##################\n",
        "################# Tweaking weights #################\n",
        "#LEARNING_RATE=1.3513402*0.1\n",
        "#del wrn_28_10\n",
        "#wrn_28_10=create_model()\n",
        "LEARNING_RATE=0.2511886*0.1\n",
        "STEPS_PER_EPOCH=30\n",
        "EPOCHS=100\n",
        "#\"/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_1577543864_1577544293_model.022.h5\"\n",
        "wrn_28_10=create_model()\n",
        "wrn_28_10.load_weights('/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_1577543864_1577544293_model.022.h5')\n",
        "loss_weights_compile = {'gender_output': 2, \n",
        "                        'image_quality_output': 2, \n",
        "                        'age_output': 6, \n",
        "                        'weight_output': 3, \n",
        "                        'bag_output': 5, \n",
        "                        'pose_output': 4, \n",
        "                        'footwear_output': 4, \n",
        "                        'emotion_output': 4}\n",
        "\n",
        "wrn_28_10.compile(\n",
        "     #optimizer=SGD(lr=1.3513402*0.1),\n",
        "     optimizer=SGD(lr=0.0001),\n",
        "     loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "     loss_weights=loss_weights_compile,\n",
        "     #weighted_metrics=[\"accuracy\"]\n",
        "     metrics=[\"accuracy\"]\n",
        ")\n",
        "callbacks = generate_new_callbacks(steps_per_epoch=STEPS_PER_EPOCH, \n",
        "                                     epoch_count=EPOCHS,\n",
        "                                     min_lr=LEARNING_RATE*0.01, \n",
        "                                     max_lr=LEARNING_RATE)\n",
        "#print(callbacks)\n",
        "wrn_28_10.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=4, \n",
        "    epochs=EPOCHS,\n",
        "    verbose=1,\n",
        "    class_weight=loss_weights_train,\n",
        "    steps_per_epoch=STEPS_PER_EPOCH,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "print(\"End of EPOCHS=\",EPOCHS,\" STEPS_PER_EPOCH=\",STEPS_PER_EPOCH)\n",
        "\n",
        "###################### loss on weight change ################\n",
        "#[-0.0207 -0.0044  0.0069  0.0108 -0.0015 -0.0098 -0.0089 -0.0103]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3KV66SLXgl7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###################### Couldn't find a good augmentation strategy hence sticking with defaults ##################\n",
        "#LEARNING_RATE=1.3513402*0.1\n",
        "#del wrn_28_10\n",
        "#wrn_28_10=create_model()\n",
        "LEARNING_RATE=0.2511886*0.1\n",
        "STEPS_PER_EPOCH=train_df.shape[0]//BATCH_SIZE\n",
        "EPOCHS=100\n",
        "#\"/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_1577543864_1577544293_model.022.h5\"\n",
        "wrn_28_10=create_model()\n",
        "wrn_28_10.load_weights('/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_1577543864_1577544293_model.022.h5')\n",
        "# loss_weights_compile = {'gender_output': 2, \n",
        "#                         'image_quality_output': 2, \n",
        "#                         'age_output': 6, \n",
        "#                         'weight_output': 3, \n",
        "#                         'bag_output': 5, \n",
        "#                         'pose_output': 4, \n",
        "#                         'footwear_output': 4, \n",
        "#                         'emotion_output': 4}\n",
        "loss_weights_compile = {'gender_output': 2, \n",
        "                        'image_quality_output': 2, \n",
        "                        'age_output': 4, \n",
        "                        'weight_output': 3, \n",
        "                        'bag_output': 3, \n",
        "                        'pose_output': 3, \n",
        "                        'footwear_output': 2, \n",
        "                        'emotion_output': 4}\n",
        "\n",
        "\n",
        "wrn_28_10.compile(\n",
        "     #optimizer=SGD(lr=1.3513402*0.1),\n",
        "     optimizer=SGD(lr=0.0001),\n",
        "     loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "     loss_weights=loss_weights_compile,\n",
        "     #weighted_metrics=[\"accuracy\"]\n",
        "     metrics=[\"accuracy\"]\n",
        ")\n",
        "callbacks = generate_new_callbacks(steps_per_epoch=STEPS_PER_EPOCH, \n",
        "                                     epoch_count=EPOCHS,\n",
        "                                     min_lr=LEARNING_RATE*0.01, \n",
        "                                     max_lr=LEARNING_RATE,\n",
        "                                   patience=15)\n",
        "#print(callbacks)\n",
        "wrn_28_10.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=4, \n",
        "    epochs=EPOCHS,\n",
        "    verbose=1,\n",
        "    #class_weight=loss_weights_train,\n",
        "    steps_per_epoch=STEPS_PER_EPOCH,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "print(\"End of EPOCHS=\",EPOCHS,\" STEPS_PER_EPOCH=\",STEPS_PER_EPOCH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNeCQCLveWBk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###################### Couldn't find a good augmentation strategy hence sticking with defaults ##################\n",
        "#LEARNING_RATE=1.3513402*0.1\n",
        "#del wrn_28_10\n",
        "#wrn_28_10=create_model()\n",
        "LEARNING_RATE=0.2511886*0.1\n",
        "STEPS_PER_EPOCH=360 #train_df.shape[0]//BATCH_SIZE\n",
        "EPOCHS=100\n",
        "#\"/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_1577543864_1577544293_model.022.h5\"\n",
        "wrn_28_10=create_model()\n",
        "#wrn_28_10.load_weights('/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_1577543864_1577544293_model.022.h5')\n",
        "wrn_28_10.load_weights('/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy_1577567748_1577569953_model.006.h5')\n",
        "# loss_weights_compile = {'gender_output': 2, \n",
        "#                         'image_quality_output': 2, \n",
        "#                         'age_output': 6, \n",
        "#                         'weight_output': 3, \n",
        "#                         'bag_output': 5, \n",
        "#                         'pose_output': 4, \n",
        "#                         'footwear_output': 4, \n",
        "#                         'emotion_output': 4}\n",
        "loss_weights_compile = {'gender_output': 2, \n",
        "                        'image_quality_output': 2, \n",
        "                        'age_output': 4, \n",
        "                        'weight_output': 3, \n",
        "                        'bag_output': 3, \n",
        "                        'pose_output': 3, \n",
        "                        'footwear_output': 2, \n",
        "                        'emotion_output': 4}\n",
        "\n",
        "\n",
        "wrn_28_10.compile(\n",
        "     #optimizer=SGD(lr=1.3513402*0.1),\n",
        "     optimizer=SGD(lr=0.0001),\n",
        "     loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "     loss_weights=loss_weights_compile,\n",
        "     #weighted_metrics=[\"accuracy\"]\n",
        "     metrics=[\"accuracy\"]\n",
        ")\n",
        "callbacks = generate_new_callbacks(steps_per_epoch=STEPS_PER_EPOCH, \n",
        "                                     epoch_count=EPOCHS,\n",
        "                                     min_lr=LEARNING_RATE*0.01, \n",
        "                                     max_lr=LEARNING_RATE,\n",
        "                                   patience=15)\n",
        "#print(callbacks)\n",
        "wrn_28_10.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=4, \n",
        "    epochs=EPOCHS,\n",
        "    verbose=1,\n",
        "    #class_weight=loss_weights_train,\n",
        "    steps_per_epoch=STEPS_PER_EPOCH,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "print(\"End of EPOCHS=\",EPOCHS,\" STEPS_PER_EPOCH=\",STEPS_PER_EPOCH)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mYSGZUstwQBm",
        "colab": {}
      },
      "source": [
        "###################### Couldn't find a good augmentation strategy hence sticking with defaults ##################\n",
        "#LEARNING_RATE=1.3513402*0.1\n",
        "#del wrn_28_10\n",
        "#wrn_28_10=create_model()\n",
        "LEARNING_RATE=0.2511886*0.1\n",
        "STEPS_PER_EPOCH=train_df.shape[0]//BATCH_SIZE\n",
        "EPOCHS=300\n",
        "#\"/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_1577543864_1577544293_model.022.h5\"\n",
        "#wrn_28_10=create_model()\n",
        "#wrn_28_10.load_weights('/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_1577543864_1577544293_model.022.h5')\n",
        "wrn_28_10.load_weights('/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_1577594233_1577594387_model.024.h5')\n",
        "# loss_weights_compile = {'gender_output': 2, \n",
        "#                         'image_quality_output': 2, \n",
        "#                         'age_output': 6, \n",
        "#                         'weight_output': 3, \n",
        "#                         'bag_output': 5, \n",
        "#                         'pose_output': 4, \n",
        "#                         'footwear_output': 4, \n",
        "#                         'emotion_output': 4}\n",
        "loss_weights_compile = {'gender_output': 2, \n",
        "                        'image_quality_output': 2, \n",
        "                        'age_output': 4, \n",
        "                        'weight_output': 3, \n",
        "                        'bag_output': 3, \n",
        "                        'pose_output': 3, \n",
        "                        'footwear_output': 2, \n",
        "                        'emotion_output': 4}\n",
        "\n",
        "\n",
        "wrn_28_10.compile(\n",
        "     #optimizer=SGD(lr=1.3513402*0.1),\n",
        "     optimizer=SGD(lr=0.0001),\n",
        "     loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "     loss_weights=loss_weights_compile,\n",
        "     #weighted_metrics=[\"accuracy\"]\n",
        "     metrics=[\"accuracy\"]\n",
        ")\n",
        "callbacks = generate_new_callbacks(steps_per_epoch=STEPS_PER_EPOCH, \n",
        "                                     epoch_count=EPOCHS,\n",
        "                                     min_lr=LEARNING_RATE*0.01, \n",
        "                                     max_lr=LEARNING_RATE,\n",
        "                                   patience=50)\n",
        "#print(callbacks)\n",
        "wrn_28_10.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=4,\n",
        "    epochs=EPOCHS,\n",
        "    verbose=1,\n",
        "    #class_weight=loss_weights_train,\n",
        "    steps_per_epoch=STEPS_PER_EPOCH,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "print(\"End of EPOCHS=\",EPOCHS,\" STEPS_PER_EPOCH=\",STEPS_PER_EPOCH)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHxLkazbVFRl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import glob\n",
        "def get_latestn_files(path = \"/content/gdrive/My Drive/WRN_Extend/\", pattern=\"*.json\",count=2):\n",
        "    #path = \"/content/gdrive/My Drive/WRN_Extend/\"\n",
        "    file_path = os.path.join(path)+pattern\n",
        "    files = [f for f in glob.iglob(file_path, recursive=False)]\n",
        "    latest_file = sorted(files, key=os.path.getctime,reverse=True)\n",
        "    return latest_file[:count]\n",
        "#files = os.listdir(path)\n",
        "#filenamere.split(r'/',files[len(files)-1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGE4A64fVL-l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "get_latestn_files(path = \"/content/gdrive/My Drive/WRN_Extend/\", pattern=\"*.h5\",count=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xw5cAATBUH0e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########## Stopped previous run on Account of overfitting ##############\n",
        "######### Start run with increased dropout\n",
        "\n",
        "###################### Couldn't find a good augmentation strategy hence sticking with defaults ##################\n",
        "#LEARNING_RATE=1.3513402*0.1\n",
        "#del wrn_28_10\n",
        "wrn_28_10=create_model(dropout=0.1)\n",
        "LEARNING_RATE=0.2511886*0.1\n",
        "STEPS_PER_EPOCH=train_df.shape[0]//BATCH_SIZE\n",
        "EPOCHS=10\n",
        "#\"/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_1577543864_1577544293_model.022.h5\"\n",
        "#wrn_28_10=create_model()\n",
        "#wrn_28_10.load_weights('/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_1577543864_1577544293_model.022.h5')\n",
        "last_saved_file = get_latestn_files(path = \"/content/gdrive/My Drive/WRN_Extend/\", pattern=\"*.h5\",count=1)\n",
        "wrn_28_10.load_weights(last_saved_file[0])\n",
        "# loss_weights_compile = {'gender_output': 2, \n",
        "#                         'image_quality_output': 2, \n",
        "#                         'age_output': 6, \n",
        "#                         'weight_output': 3, \n",
        "#                         'bag_output': 5, \n",
        "#                         'pose_output': 4, \n",
        "#                         'footwear_output': 4, \n",
        "#                         'emotion_output': 4}\n",
        "loss_weights_compile = {'gender_output': 2, \n",
        "                        'image_quality_output': 2, \n",
        "                        'age_output': 4, \n",
        "                        'weight_output': 3, \n",
        "                        'bag_output': 3, \n",
        "                        'pose_output': 3, \n",
        "                        'footwear_output': 2, \n",
        "                        'emotion_output': 4}\n",
        "\n",
        "\n",
        "wrn_28_10.compile(\n",
        "     #optimizer=SGD(lr=1.3513402*0.1),\n",
        "     optimizer=SGD(lr=0.0001),\n",
        "     loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "     loss_weights=loss_weights_compile,\n",
        "     #weighted_metrics=[\"accuracy\"]\n",
        "     metrics=[\"accuracy\"]\n",
        ")\n",
        "callbacks = generate_new_callbacks(steps_per_epoch=STEPS_PER_EPOCH, \n",
        "                                     epoch_count=EPOCHS,\n",
        "                                     min_lr=LEARNING_RATE*0.01, \n",
        "                                     max_lr=LEARNING_RATE,\n",
        "                                   patience=10)\n",
        "#print(callbacks)\n",
        "wrn_28_10.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=4,\n",
        "    epochs=EPOCHS,\n",
        "    verbose=1,\n",
        "    #class_weight=loss_weights_train,\n",
        "    steps_per_epoch=STEPS_PER_EPOCH,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "print(\"End of EPOCHS=\",EPOCHS,\" STEPS_PER_EPOCH=\",STEPS_PER_EPOCH)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBlGV4gKikGh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########## Stopped previous run on Account of overfitting ##############\n",
        "######### Start run with increased dropout\n",
        "\n",
        "###################### Couldn't find a good augmentation strategy hence sticking with defaults ##################\n",
        "#LEARNING_RATE=1.3513402*0.1\n",
        "#del wrn_28_10\n",
        "#wrn_28_10=create_model(dropout=0.1)\n",
        "LEARNING_RATE=0.2511886*0.1\n",
        "STEPS_PER_EPOCH=train_df.shape[0]//BATCH_SIZE//2\n",
        "EPOCHS=10\n",
        "#\"/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_1577543864_1577544293_model.022.h5\"\n",
        "#wrn_28_10=create_model()\n",
        "#wrn_28_10.load_weights('/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_1577543864_1577544293_model.022.h5')\n",
        "last_saved_file = get_latestn_files(path = \"/content/gdrive/My Drive/WRN_Extend/\", pattern=\"*.h5\",count=1)\n",
        "print(last_saved_file[0])\n",
        "wrn_28_10.load_weights(last_saved_file[0])\n",
        "# loss_weights_compile = {'gender_output': 2, \n",
        "#                         'image_quality_output': 2, \n",
        "#                         'age_output': 6, \n",
        "#                         'weight_output': 3, \n",
        "#                         'bag_output': 5, \n",
        "#                         'pose_output': 4, \n",
        "#                         'footwear_output': 4, \n",
        "#                         'emotion_output': 4}\n",
        "loss_weights_compile = {'gender_output': 2, \n",
        "                        'image_quality_output': 2, \n",
        "                        'age_output': 4, \n",
        "                        'weight_output': 3, \n",
        "                        'bag_output': 3, \n",
        "                        'pose_output': 3, \n",
        "                        'footwear_output': 2, \n",
        "                        'emotion_output': 4}\n",
        "\n",
        "\n",
        "wrn_28_10.compile(\n",
        "     #optimizer=SGD(lr=1.3513402*0.1),\n",
        "     optimizer=SGD(lr=0.0001),\n",
        "     loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "     loss_weights=loss_weights_compile,\n",
        "     #weighted_metrics=[\"accuracy\"]\n",
        "     metrics=[\"accuracy\"]\n",
        ")\n",
        "callbacks = generate_new_callbacks(steps_per_epoch=STEPS_PER_EPOCH, \n",
        "                                     epoch_count=EPOCHS,\n",
        "                                     min_lr=LEARNING_RATE*0.01, \n",
        "                                     max_lr=LEARNING_RATE,\n",
        "                                   patience=10)\n",
        "#print(callbacks)\n",
        "wrn_28_10.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=4,\n",
        "    epochs=EPOCHS,\n",
        "    verbose=1,\n",
        "    #class_weight=loss_weights_train,\n",
        "    steps_per_epoch=STEPS_PER_EPOCH,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "print(\"End of EPOCHS=\",EPOCHS,\" STEPS_PER_EPOCH=\",STEPS_PER_EPOCH)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGswX2HOosur",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########## Stopped previous run on Account of overfitting ##############\n",
        "######### Start run with increased dropout\n",
        "\n",
        "###################### Couldn't find a good augmentation strategy hence sticking with defaults ##################\n",
        "#LEARNING_RATE=1.3513402*0.1\n",
        "#del wrn_28_10\n",
        "#wrn_28_10=create_model(dropout=0.1)\n",
        "LEARNING_RATE=0.2511886*0.1\n",
        "STEPS_PER_EPOCH=train_df.shape[0]//BATCH_SIZE//2\n",
        "EPOCHS=20\n",
        "#\"/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_1577543864_1577544293_model.022.h5\"\n",
        "#wrn_28_10=create_model()\n",
        "#wrn_28_10.load_weights('/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_1577543864_1577544293_model.022.h5')\n",
        "last_saved_file = get_latestn_files(path = \"/content/gdrive/My Drive/WRN_Extend/\", pattern=\"*.h5\",count=1)\n",
        "print(last_saved_file[0])\n",
        "wrn_28_10.load_weights(last_saved_file[0])\n",
        "# loss_weights_compile = {'gender_output': 2, \n",
        "#                         'image_quality_output': 2, \n",
        "#                         'age_output': 6, \n",
        "#                         'weight_output': 3, \n",
        "#                         'bag_output': 5, \n",
        "#                         'pose_output': 4, \n",
        "#                         'footwear_output': 4, \n",
        "#                         'emotion_output': 4}\n",
        "loss_weights_compile = {'gender_output': 2, \n",
        "                        'image_quality_output': 2, \n",
        "                        'age_output': 4, \n",
        "                        'weight_output': 3, \n",
        "                        'bag_output': 3, \n",
        "                        'pose_output': 3, \n",
        "                        'footwear_output': 2, \n",
        "                        'emotion_output': 4}\n",
        "\n",
        "\n",
        "wrn_28_10.compile(\n",
        "     #optimizer=SGD(lr=1.3513402*0.1),\n",
        "     optimizer=SGD(lr=0.0001),\n",
        "     loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "     loss_weights=loss_weights_compile,\n",
        "     #weighted_metrics=[\"accuracy\"]\n",
        "     metrics=[\"accuracy\"]\n",
        ")\n",
        "callbacks = generate_new_callbacks(steps_per_epoch=STEPS_PER_EPOCH, \n",
        "                                     epoch_count=EPOCHS,\n",
        "                                     min_lr=LEARNING_RATE*0.01, \n",
        "                                     max_lr=LEARNING_RATE,\n",
        "                                   patience=10)\n",
        "#print(callbacks)\n",
        "wrn_28_10.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=4,\n",
        "    epochs=EPOCHS,\n",
        "    verbose=1,\n",
        "    #class_weight=loss_weights_train,\n",
        "    steps_per_epoch=STEPS_PER_EPOCH,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "print(\"End of EPOCHS=\",EPOCHS,\" STEPS_PER_EPOCH=\",STEPS_PER_EPOCH)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t887bmlTyBSw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "get_latestn_files(path = \"/content/gdrive/My Drive/WRN_Extend/\", pattern=\"*.h5\",count=20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pXD3J4ex-z2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########## Stopped previous run on Account of overfitting ##############\n",
        "######### Start run with increased dropout\n",
        "\n",
        "###################### Couldn't find a good augmentation strategy hence sticking with defaults ##################\n",
        "#LEARNING_RATE=1.3513402*0.1\n",
        "#del wrn_28_10\n",
        "wrn_28_10=create_model(dropout=0.05)\n",
        "LEARNING_RATE=0.2511886*0.1\n",
        "STEPS_PER_EPOCH=train_df.shape[0]//BATCH_SIZE//2\n",
        "EPOCHS=20\n",
        "#\"/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_1577543864_1577544293_model.022.h5\"\n",
        "#wrn_28_10=create_model()\n",
        "#wrn_28_10.load_weights('/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_1577543864_1577544293_model.022.h5')\n",
        "last_saved_file = '/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_1577594233_1577603312_model.048.h5'\n",
        "#get_latestn_files(path = \"/content/gdrive/My Drive/WRN_Extend/\", pattern=\"*.h5\",count=1)\n",
        "print(last_saved_file)\n",
        "wrn_28_10.load_weights(last_saved_file)\n",
        "# loss_weights_compile = {'gender_output': 2, \n",
        "#                         'image_quality_output': 2, \n",
        "#                         'age_output': 6, \n",
        "#                         'weight_output': 3, \n",
        "#                         'bag_output': 5, \n",
        "#                         'pose_output': 4, \n",
        "#                         'footwear_output': 4, \n",
        "#                         'emotion_output': 4}\n",
        "loss_weights_compile = {'gender_output': 2, \n",
        "                        'image_quality_output': 2, \n",
        "                        'age_output': 4, \n",
        "                        'weight_output': 3, \n",
        "                        'bag_output': 3, \n",
        "                        'pose_output': 3, \n",
        "                        'footwear_output': 2, \n",
        "                        'emotion_output': 4}\n",
        "\n",
        "\n",
        "wrn_28_10.compile(\n",
        "     #optimizer=SGD(lr=1.3513402*0.1),\n",
        "     optimizer=SGD(lr=0.0001),\n",
        "     loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "     loss_weights=loss_weights_compile,\n",
        "     #weighted_metrics=[\"accuracy\"]\n",
        "     metrics=[\"accuracy\"]\n",
        ")\n",
        "callbacks = generate_new_callbacks(steps_per_epoch=STEPS_PER_EPOCH, \n",
        "                                     epoch_count=EPOCHS,\n",
        "                                     min_lr=LEARNING_RATE*0.01, \n",
        "                                     max_lr=LEARNING_RATE,\n",
        "                                   patience=10)\n",
        "#print(callbacks)\n",
        "wrn_28_10.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=4,\n",
        "    epochs=EPOCHS,\n",
        "    verbose=1,\n",
        "    #class_weight=loss_weights_train,\n",
        "    steps_per_epoch=STEPS_PER_EPOCH,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "print(\"End of EPOCHS=\",EPOCHS,\" STEPS_PER_EPOCH=\",STEPS_PER_EPOCH)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcVeHFyq-HcC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wrn_28_10.save('/content/gdrive/My Drive/WRN_Extend/Latest_val_loss_19924306.h5py')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMOGPX7I9l1o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########## Loss improved to 19.924306 ##############\n",
        "\n",
        "\n",
        "################################# Below are multiple Trial and Errors ######################\n",
        "################################# No big difference observed in the results ###############################\n",
        "######### Trying with clr param changes clr_mode='triangular2', clr_multiplier=8\n",
        "\n",
        "###################### Couldn't find a good augmentation strategy hence sticking with defaults ##################\n",
        "#LEARNING_RATE=1.3513402*0.1\n",
        "#del wrn_28_10\n",
        "#wrn_28_10=create_model(dropout=0.05)\n",
        "LEARNING_RATE=0.2511886*0.1\n",
        "STEPS_PER_EPOCH=train_df.shape[0]//BATCH_SIZE//2\n",
        "EPOCHS=20\n",
        "#\"/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_1577543864_1577544293_model.022.h5\"\n",
        "#wrn_28_10=create_model()\n",
        "#wrn_28_10.load_weights('/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_1577543864_1577544293_model.022.h5')\n",
        "last_saved_file = '/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_gml2_1577624977_1577625577_model.008.h5'\n",
        "#get_latestn_files(path = \"/content/gdrive/My Drive/WRN_Extend/\", pattern=\"*.h5\",count=1)\n",
        "print(last_saved_file)\n",
        "wrn_28_10.load_weights(last_saved_file)\n",
        "# loss_weights_compile = {'gender_output': 2, \n",
        "#                         'image_quality_output': 2, \n",
        "#                         'age_output': 6, \n",
        "#                         'weight_output': 3, \n",
        "#                         'bag_output': 5, \n",
        "#                         'pose_output': 4, \n",
        "#                         'footwear_output': 4, \n",
        "#                         'emotion_output': 4}\n",
        "loss_weights_compile = {'gender_output': 2, \n",
        "                        'image_quality_output': 2, \n",
        "                        'age_output': 4, \n",
        "                        'weight_output': 3, \n",
        "                        'bag_output': 3, \n",
        "                        'pose_output': 3, \n",
        "                        'footwear_output': 2, \n",
        "                        'emotion_output': 4}\n",
        "\n",
        "\n",
        "wrn_28_10.compile(\n",
        "     #optimizer=SGD(lr=1.3513402*0.1),\n",
        "     optimizer=SGD(lr=0.0001),\n",
        "     loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "     loss_weights=loss_weights_compile,\n",
        "     #weighted_metrics=[\"accuracy\"]\n",
        "     metrics=[\"accuracy\"]\n",
        ")\n",
        "callbacks = generate_new_callbacks(steps_per_epoch=STEPS_PER_EPOCH, \n",
        "                                     epoch_count=EPOCHS,\n",
        "                                     min_lr=LEARNING_RATE*0.01, \n",
        "                                     max_lr=LEARNING_RATE,\n",
        "                                   patience=10,\n",
        "                                   clr_mode='triangular2',\n",
        "                                   clr_multiplier=8)\n",
        "#print(callbacks)\n",
        "wrn_28_10.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=4,\n",
        "    epochs=EPOCHS,\n",
        "    verbose=1,\n",
        "    #class_weight=loss_weights_train,\n",
        "    steps_per_epoch=STEPS_PER_EPOCH,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "print(\"End of EPOCHS=\",EPOCHS,\" STEPS_PER_EPOCH=\",STEPS_PER_EPOCH)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBMGW_uANYbs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########## Loss improved to 19.924306 ##############\n",
        "######### Triangular2 didnt give much benefit #### \n",
        "######### Fallback to triangular with multiplier 4\n",
        "\n",
        "###################### Couldn't find a good augmentation strategy hence sticking with defaults ##################\n",
        "#LEARNING_RATE=1.3513402*0.1\n",
        "#del wrn_28_10\n",
        "#wrn_28_10=create_model(dropout=0.05)\n",
        "LEARNING_RATE=0.2511886*0.1\n",
        "STEPS_PER_EPOCH=train_df.shape[0]//BATCH_SIZE//2\n",
        "EPOCHS=30\n",
        "#\"/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_1577543864_1577544293_model.022.h5\"\n",
        "#wrn_28_10=create_model()\n",
        "#wrn_28_10.load_weights('/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_1577543864_1577544293_model.022.h5')\n",
        "last_saved_file = '/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_gml2_1577624977_1577625577_model.008.h5'\n",
        "#get_latestn_files(path = \"/content/gdrive/My Drive/WRN_Extend/\", pattern=\"*.h5\",count=1)\n",
        "print(last_saved_file)\n",
        "wrn_28_10.load_weights(last_saved_file)\n",
        "# loss_weights_compile = {'gender_output': 2, \n",
        "#                         'image_quality_output': 2, \n",
        "#                         'age_output': 6, \n",
        "#                         'weight_output': 3, \n",
        "#                         'bag_output': 5, \n",
        "#                         'pose_output': 4, \n",
        "#                         'footwear_output': 4, \n",
        "#                         'emotion_output': 4}\n",
        "loss_weights_compile = {'gender_output': 2, \n",
        "                        'image_quality_output': 2, \n",
        "                        'age_output': 4, \n",
        "                        'weight_output': 3, \n",
        "                        'bag_output': 3, \n",
        "                        'pose_output': 3, \n",
        "                        'footwear_output': 2, \n",
        "                        'emotion_output': 4}\n",
        "\n",
        "\n",
        "wrn_28_10.compile(\n",
        "     #optimizer=SGD(lr=1.3513402*0.1),\n",
        "     optimizer=SGD(lr=0.0001),\n",
        "     loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "     loss_weights=loss_weights_compile,\n",
        "     #weighted_metrics=[\"accuracy\"]\n",
        "     metrics=[\"accuracy\"]\n",
        ")\n",
        "callbacks = generate_new_callbacks(steps_per_epoch=STEPS_PER_EPOCH, \n",
        "                                     epoch_count=EPOCHS,\n",
        "                                     min_lr=LEARNING_RATE*0.01, \n",
        "                                     max_lr=LEARNING_RATE,\n",
        "                                   patience=10,\n",
        "                                   #clr_mode='triangular2',\n",
        "                                   clr_multiplier=4)\n",
        "#print(callbacks)\n",
        "wrn_28_10.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=4,\n",
        "    epochs=EPOCHS,\n",
        "    verbose=1,\n",
        "    #class_weight=loss_weights_train,\n",
        "    steps_per_epoch=STEPS_PER_EPOCH,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "print(\"End of EPOCHS=\",EPOCHS,\" STEPS_PER_EPOCH=\",STEPS_PER_EPOCH)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZJ7uq_nZ5Ub",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########## Loss improved to 19.924306 ##############\n",
        "######### Triangular2 didnt give much benefit #### \n",
        "######### Fallback to triangular with multiplier 4\n",
        "\n",
        "###################### Couldn't find a good augmentation strategy hence sticking with defaults ##################\n",
        "#LEARNING_RATE=1.3513402*0.1\n",
        "#del wrn_28_10\n",
        "#wrn_28_10=create_model(dropout=0.05)\n",
        "LEARNING_RATE=0.2511886*0.1\n",
        "STEPS_PER_EPOCH=train_df.shape[0]//BATCH_SIZE//2\n",
        "EPOCHS=20\n",
        "#\"/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_1577543864_1577544293_model.022.h5\"\n",
        "#wrn_28_10=create_model()\n",
        "#wrn_28_10.load_weights('/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_1577543864_1577544293_model.022.h5')\n",
        "last_saved_file = '/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_gml2_1577624977_1577625577_model.008.h5'\n",
        "#last_saved_file ='/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_gml2_1577624977_1577632609_model.008.h5'\n",
        "#get_latestn_files(path = \"/content/gdrive/My Drive/WRN_Extend/\", pattern=\"*.h5\",count=1)\n",
        "print(last_saved_file)\n",
        "wrn_28_10.load_weights(last_saved_file)\n",
        "# loss_weights_compile = {'gender_output': 2, \n",
        "#                         'image_quality_output': 2, \n",
        "#                         'age_output': 6, \n",
        "#                         'weight_output': 3, \n",
        "#                         'bag_output': 5, \n",
        "#                         'pose_output': 4, \n",
        "#                         'footwear_output': 4, \n",
        "#                         'emotion_output': 4}\n",
        "loss_weights_compile = {'gender_output': 1, \n",
        "                        'image_quality_output': 2, \n",
        "                        'age_output': 2, \n",
        "                        'weight_output': 1, \n",
        "                        'bag_output': 2, \n",
        "                        'pose_output': 1, \n",
        "                        'footwear_output': 2, \n",
        "                        'emotion_output': 4}\n",
        "\n",
        "\n",
        "wrn_28_10.compile(\n",
        "     #optimizer=SGD(lr=1.3513402*0.1),\n",
        "     optimizer=SGD(lr=0.0001),\n",
        "     loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "     loss_weights=loss_weights_compile,\n",
        "     #weighted_metrics=[\"accuracy\"]\n",
        "     metrics=[\"accuracy\"]\n",
        ")\n",
        "callbacks = generate_new_callbacks(steps_per_epoch=STEPS_PER_EPOCH, \n",
        "                                     epoch_count=EPOCHS,\n",
        "                                     min_lr=LEARNING_RATE*0.01, \n",
        "                                     max_lr=LEARNING_RATE,\n",
        "                                   patience=10,\n",
        "                                   #clr_mode='triangular2',\n",
        "                                   clr_multiplier=4)\n",
        "#print(callbacks)\n",
        "wrn_28_10.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=4,\n",
        "    epochs=EPOCHS,\n",
        "    verbose=1,\n",
        "    #class_weight=loss_weights_train,\n",
        "    steps_per_epoch=STEPS_PER_EPOCH,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "print(\"End of EPOCHS=\",EPOCHS,\" STEPS_PER_EPOCH=\",STEPS_PER_EPOCH)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALFXF7ZPpwkO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########## Loss improved to 19.924306 ##############\n",
        "######### Triangular2 didnt give much benefit #### \n",
        "######### Fallback to triangular with multiplier 4\n",
        "\n",
        "###################### Couldn't find a good augmentation strategy hence sticking with defaults ##################\n",
        "#LEARNING_RATE=1.3513402*0.1\n",
        "#del wrn_28_10\n",
        "#wrn_28_10=create_model(dropout=0.05)\n",
        "LEARNING_RATE=0.2511886*0.1\n",
        "STEPS_PER_EPOCH=train_df.shape[0]//BATCH_SIZE//2\n",
        "EPOCHS=20\n",
        "#\"/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_1577543864_1577544293_model.022.h5\"\n",
        "#wrn_28_10=create_model()\n",
        "#wrn_28_10.load_weights('/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_1577543864_1577544293_model.022.h5')\n",
        "last_saved_file = '/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_gml2_1577624977_1577637125_model.016.h5'\n",
        "#last_saved_file ='/content/gdrive/My Drive/WRN_Extend/assignment5_wrn2_rkg_fresh_wrn_wide_rjy2_gml2_1577624977_1577632609_model.008.h5'\n",
        "#get_latestn_files(path = \"/content/gdrive/My Drive/WRN_Extend/\", pattern=\"*.h5\",count=1)\n",
        "print(last_saved_file)\n",
        "wrn_28_10.load_weights(last_saved_file)\n",
        "# loss_weights_compile = {'gender_output': 2, \n",
        "#                         'image_quality_output': 2, \n",
        "#                         'age_output': 6, \n",
        "#                         'weight_output': 3, \n",
        "#                         'bag_output': 5, \n",
        "#                         'pose_output': 4, \n",
        "#                         'footwear_output': 4, \n",
        "#                         'emotion_output': 4}\n",
        "loss_weights_compile = {'gender_output': 1, \n",
        "                        'image_quality_output': 2, \n",
        "                        'age_output': 2, \n",
        "                        'weight_output': 1, \n",
        "                        'bag_output': 2, \n",
        "                        'pose_output': 1, \n",
        "                        'footwear_output': 2, \n",
        "                        'emotion_output': 4}\n",
        "\n",
        "\n",
        "wrn_28_10.compile(\n",
        "     #optimizer=SGD(lr=1.3513402*0.1),\n",
        "     optimizer=SGD(lr=0.0001),\n",
        "     loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "     loss_weights=loss_weights_compile,\n",
        "     #weighted_metrics=[\"accuracy\"]\n",
        "     metrics=[\"accuracy\"]\n",
        ")\n",
        "callbacks = generate_new_callbacks(steps_per_epoch=STEPS_PER_EPOCH, \n",
        "                                     epoch_count=EPOCHS,\n",
        "                                     min_lr=LEARNING_RATE*0.01, \n",
        "                                     max_lr=LEARNING_RATE,\n",
        "                                   patience=10,\n",
        "                                   #clr_mode='triangular2',\n",
        "                                   clr_multiplier=4)\n",
        "#print(callbacks)\n",
        "wrn_28_10.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=4,\n",
        "    epochs=EPOCHS,\n",
        "    verbose=1,\n",
        "    #class_weight=loss_weights_train,\n",
        "    steps_per_epoch=STEPS_PER_EPOCH,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "print(\"End of EPOCHS=\",EPOCHS,\" STEPS_PER_EPOCH=\",STEPS_PER_EPOCH)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lsZxVEwBTWe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp \"/content/gdrive/My Drive/json_wrn2_widrn_acc_1577202722.json\" \"/content/gdrive/My Drive/json_wrn2_widrn_acc_1577202722_round2.json\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vx79InfALyOx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wrn_28_10.save(weights_file+\"py\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwUEJewRFxtf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "y_pred_results = wrn_28_10.predict_generator(valid_gen, (2036 // 32+1),verbose=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BespGPlFHKHx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred_dict['val_age_output_acc'].shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-39qKvhHFHB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred_dict_new=get_indexed_results(y_pred_results)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vt2VU5EiI1dC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "def get_indexed_results(y_pred_results):\n",
        "    class_output_arr = ['gender',\n",
        "                        'imagequality',\n",
        "                        'age',\n",
        "                        'weight',\n",
        "                        'carryingbag',\n",
        "                        'bodypose',    \n",
        "                        'footwear',   \n",
        "                        'emotion']\n",
        "    y_pred_dict = {}\n",
        "    for len_val in range(len(y_pred_results)):\n",
        "        y_pred_local = y_pred_results[len_val][:-12]\n",
        "        y_pred_final = np.argmax(y_pred_local, axis=1)\n",
        "        y_pred_dict[class_output_arr[len_val]]=y_pred_final\n",
        "    return y_pred_dict\n",
        "def print_confusion_matrix(y_pred_dict, val_df_to_use, attribute_to_select):\n",
        "    cols_to_select=[col for col in one_hot_df.columns if col.startswith(attribute_to_select)]\n",
        "    y_true = val_df_to_use[cols_to_select].values\n",
        "    y_true_classes = np.argmax(y_true, axis=1)\n",
        "    y_pred=y_pred_dict[attribute_to_select]\n",
        "    #print(y_pred_results[].shape, y_true.shape)\n",
        "    #print(y_true_classes)\n",
        "    y_true_classes = np.argmax(y_true, axis=1)\n",
        "    print(\"Confusion Matrix for\", attribute_to_select)\n",
        "    matrix = confusion_matrix(y_true_classes, y_pred)\n",
        "    print(matrix)\n",
        "    true_class_dist = [ np.where( y_true_classes==classes)[0].shape[0] for classes in np.unique(y_true_classes)]\n",
        "    print(\"True Class Dist\",true_class_dist)\n",
        "    pred_class_dist = [ np.where( y_pred==classes)[0].shape[0] for classes in np.unique(y_pred)]\n",
        "    print(\"Predicted class dist\",pred_class_dist)\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "y_pred_results = wrn_28_10.predict_generator(valid_gen, (2036 // 32+1),verbose=True)\n",
        "\n",
        "y_pred_dict_new=get_indexed_results(y_pred_results)\n",
        "for value_col in range(len(class_output_arr)):\n",
        "    print_confusion_matrix(y_pred_dict_new, val_df, class_output_arr[value_col])\n",
        "    print(\"***************\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvBCk7s2M62K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for value_col in range(len(class_output_arr)):\n",
        "    print_confusion_matrix(y_pred_dict_new, val_df, class_output_arr[value_col])\n",
        "    print(\"***************\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZNRMbh8Jy4X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred=y_pred_dict['val_emotion_output_acc']\n",
        "true_class_dist = [ np.where( y_true_classes==classes)[0].shape[0] for classes in np.unique(y_true_classes)]\n",
        "print(\"True Class Dist\",true_class_dist)\n",
        "pred_class_dist = [ np.where( y_pred==classes)[0].shape[0] for classes in np.unique(y_pred)]\n",
        "print(\"Predicted class dist\",pred_class_dist)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpkFGgymGq47",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_gender_output_acc   \n",
        "val_image_quality_output_acc    \n",
        "val_age_output_acc    \n",
        "val_weight_output_acc    \n",
        "val_bag_output_acc   \n",
        "val_pose_output_acc    \n",
        "val_footwear_output_acc   \n",
        "val_emotion_output_acc\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zI1hJb4qM6OH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import math\n",
        "from keras.callbacks import LambdaCallback\n",
        "import keras.backend as K\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class LRFinder_new:\n",
        "    \"\"\"\n",
        "    Plots the change of the loss function of a Keras model when the learning rate is exponentially increasing.\n",
        "    See for details:\n",
        "    https://towardsdatascience.com/estimating-optimal-learning-rate-for-a-deep-neural-network-ce32f2556ce0\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "        self.losses = []\n",
        "        self.lrs = []\n",
        "        self.best_loss = 1e9\n",
        "\n",
        "    def on_batch_end(self, batch, logs):\n",
        "        # Log the learning rate\n",
        "        lr = K.get_value(self.model.optimizer.lr)\n",
        "        self.lrs.append(lr)\n",
        "\n",
        "        # Log the loss\n",
        "        loss = logs['loss']\n",
        "        self.losses.append(loss)\n",
        "\n",
        "        # Check whether the loss got too large or NaN\n",
        "        if batch > 5 and (math.isnan(loss) or loss > self.best_loss * 4):\n",
        "            print(\"\")\n",
        "            print(\"Training stopped due to high loss\",loss)\n",
        "            self.model.stop_training = True\n",
        "            return\n",
        "\n",
        "        if loss < self.best_loss:\n",
        "            self.best_loss = loss\n",
        "\n",
        "        # Increase the learning rate for the next batch\n",
        "        lr *= self.lr_mult\n",
        "        K.set_value(self.model.optimizer.lr, lr)\n",
        "\n",
        "    def find(self, x_train, y_train, start_lr, end_lr, batch_size=64, epochs=1):\n",
        "        # If x_train contains data for multiple inputs, use length of the first input.\n",
        "        # Assumption: the first element in the list is single input; NOT a list of inputs.\n",
        "        N = x_train[0].shape[0] if isinstance(x_train, list) else x_train.shape[0]\n",
        "\n",
        "        # Compute number of batches and LR multiplier\n",
        "        num_batches = epochs * N / batch_size\n",
        "        self.lr_mult = (float(end_lr) / float(start_lr)) ** (float(1) / float(num_batches))\n",
        "        # Save weights into a file\n",
        "        self.model.save_weights('tmp.h5')\n",
        "\n",
        "        # Remember the original learning rate\n",
        "        original_lr = K.get_value(self.model.optimizer.lr)\n",
        "\n",
        "        # Set the initial learning rate\n",
        "        K.set_value(self.model.optimizer.lr, start_lr)\n",
        "\n",
        "        callback = LambdaCallback(on_batch_end=lambda batch, logs: self.on_batch_end(batch, logs))\n",
        "\n",
        "        self.model.fit(x_train, y_train,\n",
        "                       batch_size=batch_size, epochs=epochs,\n",
        "                       callbacks=[callback])\n",
        "\n",
        "        # Restore the weights to the state before model fitting\n",
        "        self.model.load_weights('tmp.h5')\n",
        "\n",
        "        # Restore the original learning rate\n",
        "        K.set_value(self.model.optimizer.lr, original_lr)\n",
        "\n",
        "    def find_generator(self, generator, start_lr, end_lr, epochs=1, steps_per_epoch=None, **kw_fit):\n",
        "        if steps_per_epoch is None:\n",
        "            try:\n",
        "                steps_per_epoch = len(generator)\n",
        "            except (ValueError, NotImplementedError) as e:\n",
        "                raise e('`steps_per_epoch=None` is only valid for a'\n",
        "                        ' generator based on the '\n",
        "                        '`keras.utils.Sequence`'\n",
        "                        ' class. Please specify `steps_per_epoch` '\n",
        "                        'or use the `keras.utils.Sequence` class.')\n",
        "        self.lr_mult = (float(end_lr) / float(start_lr)) ** (float(1) / float(epochs * steps_per_epoch))\n",
        "\n",
        "        # Save weights into a file\n",
        "        self.model.save_weights('tmp.h5')\n",
        "\n",
        "        # Remember the original learning rate\n",
        "        original_lr = K.get_value(self.model.optimizer.lr)\n",
        "\n",
        "        # Set the initial learning rate\n",
        "        K.set_value(self.model.optimizer.lr, start_lr)\n",
        "\n",
        "        callback = LambdaCallback(on_batch_end=lambda batch,\n",
        "                                                      logs: self.on_batch_end(batch, logs))\n",
        "\n",
        "        self.model.fit_generator(generator=generator,\n",
        "                                 epochs=epochs,\n",
        "                                 steps_per_epoch=steps_per_epoch,\n",
        "                                 callbacks=[callback],\n",
        "                                 **kw_fit)\n",
        "\n",
        "        # Restore the weights to the state before model fitting\n",
        "        self.model.load_weights('tmp.h5')\n",
        "\n",
        "        # Restore the original learning rate\n",
        "        K.set_value(self.model.optimizer.lr, original_lr)\n",
        "\n",
        "    def plot_loss(self, n_skip_beginning=10, n_skip_end=5, x_scale='log'):\n",
        "        \"\"\"\n",
        "        Plots the loss.\n",
        "        Parameters:\n",
        "            n_skip_beginning - number of batches to skip on the left.\n",
        "            n_skip_end - number of batches to skip on the right.\n",
        "        \"\"\"\n",
        "        plt.ylabel(\"loss\")\n",
        "        plt.xlabel(\"learning rate (log scale)\")\n",
        "        plt.plot(self.lrs[n_skip_beginning:-n_skip_end], self.losses[n_skip_beginning:-n_skip_end])\n",
        "        plt.xscale(x_scale)\n",
        "        plt.show()\n",
        "\n",
        "    def plot_loss_change(self, sma=1, n_skip_beginning=10, n_skip_end=5, y_lim=(-0.01, 0.01)):\n",
        "        \"\"\"\n",
        "        Plots rate of change of the loss function.\n",
        "        Parameters:\n",
        "            sma - number of batches for simple moving average to smooth out the curve.\n",
        "            n_skip_beginning - number of batches to skip on the left.\n",
        "            n_skip_end - number of batches to skip on the right.\n",
        "            y_lim - limits for the y axis.\n",
        "        \"\"\"\n",
        "        derivatives = self.get_derivatives(sma)[n_skip_beginning:-n_skip_end]\n",
        "        lrs = self.lrs[n_skip_beginning:-n_skip_end]\n",
        "        plt.ylabel(\"rate of loss change\")\n",
        "        plt.xlabel(\"learning rate (log scale)\")\n",
        "        plt.plot(lrs, derivatives)\n",
        "        plt.xscale('log')\n",
        "        plt.ylim(y_lim)\n",
        "        plt.show()\n",
        "\n",
        "    def get_derivatives(self, sma):\n",
        "        assert sma >= 1\n",
        "        derivatives = [0] * sma\n",
        "        for i in range(sma, len(self.lrs)):\n",
        "            derivatives.append((self.losses[i] - self.losses[i - sma]) / sma)\n",
        "        return derivatives\n",
        "\n",
        "    def get_best_lr(self, sma, n_skip_beginning=10, n_skip_end=5):\n",
        "        derivatives = self.get_derivatives(sma)\n",
        "        best_der_idx = np.argmax(derivatives[n_skip_beginning:-n_skip_end])\n",
        "        return self.lrs[n_skip_beginning:-n_skip_end][best_der_idx]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIcGpeISVW0v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}